{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CkandT/Colab/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hCXK5o17RHxJ"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cL8hy5W3RHxK"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XJbav5RHxL"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f8pDlFsDRHxL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZULztBcRHxL"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vpzDxbiYRHxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd15ad1d-7945-4f2d-e346-8291a5ba3a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 98 (delta 19), reused 52 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 8.98 MiB | 23.63 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Collecting loguru (from -r requirements.txt (line 3))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.3)\n",
            "Collecting trl>=0.15.2 (from -r requirements.txt (line 10))\n",
            "  Downloading trl-0.26.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2 (from -r requirements.txt (line 13))\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended->-r requirements.txt (line 12))\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.26.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime, loguru, latex2sympy2_extended, math-verify, trl\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.0.6 loguru-0.7.3 math-verify-0.5.2 trl-0.26.1\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5df53U6b6pj",
        "outputId": "338d601b-b33d-4304-edbd-5cbebcc489b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.18.0\n",
            "✅ Transformers版本: 4.57.3\n",
            "✅ TRL版本: 0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.49.0 peft==0.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UP9JyUKOdKpP",
        "outputId": "db62e47a-220f-4fbc-bf17-1f60615a0519"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.49.0\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.14.0\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (1.12.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (3.0.3)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trl 0.26.1 requires transformers>=4.56.1, but you have transformers 4.49.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.14.0 tokenizers-0.21.4 transformers-4.49.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "peft",
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "7de54562e29148adaaf30c0f328bc777"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCbPPkptdcr2",
        "outputId": "8f8a7d07-e813-446c-ae71-273009cbf3d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.14.0\n",
            "✅ Transformers版本: 4.49.0\n",
            "✅ TRL版本: 0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_KL15Aowdswo",
        "outputId": "51335e03-5d05-4839-c991-dfd061d426c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/MedicalGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYC6lC9Ld4nJ",
        "outputId": "3938f08b-4f21-432b-da59-c94e7ea98425"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MedicalGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo1r9sL3RHxM"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I8EhYMc7RHxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fefa3c-7e47-4ca0-8968-cdfea219106f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1gy_iFL9RHxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44dcfc94-c0a7-4e06-f149-6677a0fe3fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:22:33.628705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765876953.648313    3655 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765876953.654346    3655 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765876953.669526    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669551    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669555    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669560    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-16 09:22:39.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Dec16_09-22-38_913ef8db9873,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 7.23kB [00:00, 30.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 71.3MB/s]\n",
            "merges.txt: 1.67MB [00:00, 145MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 184MB/s]\n",
            "\u001b[32m2025-12-16 09:22:40.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:40.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 235480.78 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 412188.39 examples/s]\n",
            "\u001b[32m2025-12-16 09:22:40.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 364.41 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 371.55 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9738.12 examples/s] \n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9488.42 examples/s] \n",
            "\u001b[32m2025-12-16 09:23:07.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.893\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.894\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 5.36MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:06<00:00, 164MB/s]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 998kB/s]\n",
            "\u001b[32m2025-12-16 09:23:15.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py:658: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-12-16 09:23:16.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:16.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[100673, 109838, 116545, 105252,   1773, 116124, 118439, 112140,  42140,\n",
            "          23031,  99931, 101232, 100439, 100782,   3837,  99931, 100815,  99178,\n",
            "           5373,  99243,  99178,  33108,  99931,  99251,  30440,  99726,  74040,\n",
            "         105178, 112821,   3837,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  55806,  22243,  33071,  74040,   1773,  63109, 101408, 100439,\n",
            "          42140,  17714,  17881,  99178,  33071,  63109, 101408, 100439,   3837,\n",
            "          63109, 101408,  99877,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  63109, 101408, 105753, 117908,   5373,  74040,  33071,   5373,\n",
            "         100250,  99561,   3837,  63109,  67279, 100439,  33071, 101595,  99842,\n",
            "         100636, 100347, 102914,  20221,  33071,  63109,  67279, 100439,  49567,\n",
            "           1773, 109861, 100771, 101364, 100439,   5373, 119386, 101002, 100439,\n",
            "           5373, 101335, 110776, 100439,   5373, 102512, 100439,   5373, 101743,\n",
            "         120449, 100439,   3837, 100636,  99873,  81217, 119386,  99825, 102150,\n",
            "         100771,     40,  24300, 107218,   3837,  87267,  57218, 101320,   9370,\n",
            "         101041, 106722],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [101232, 104204, 103118,  52853,   3837,  29524, 101631, 100517, 108735,\n",
            "           5373,  99180,  35551, 109167, 101232, 101913, 101538,  99676,  49567,\n",
            "          24968, 101034, 101979, 114194,   3837,  29524,  99389,  89481,  99931,\n",
            "         114194,  33108, 100521,  99665, 114194,  49567,   8997,     17,     13,\n",
            "         116711, 100154,  23990,  71137,  89481, 116711, 102150,  72448, 100630,\n",
            "         102713, 101047,  82894,  99314,  26288,  23990,  71137, 102150,   3837,\n",
            "         101364,   5373, 101696,   5373, 114814,  36885,   5373, 100049, 103661,\n",
            "          15946,  99549,  99996, 116711, 102150,  33108, 100646, 101425, 102150,\n",
            "           9909, 104033,  15946,  33071, 101425, 102150,  74276, 104017,  71268,\n",
            "         100629,  65676,  65278, 112880, 116711,  98380,   3837,  30440, 107727,\n",
            "         111936, 102595,  99252,  52129,  31914,   8997,     18,  31914, 100058,\n",
            "         111841, 100630, 114672,  31914, 100058, 101047,  99622,  31914,   5373,\n",
            "         101631, 100517, 108735,      7,    398,    704,  20773,     68,  21280,\n",
            "            225, 101538,  54926, 102835,    955,    579,   2248,    439,    258,\n",
            "          21280,    225]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[100673, 109838, 116545, 105252,   1773, 116124, 118439, 112140,  42140,\n",
            "          23031,  99931, 101232, 100439, 100782,   3837,  99931, 100815,  99178,\n",
            "           5373,  99243,  99178,  33108,  99931,  99251,  30440,  99726,  74040,\n",
            "         105178, 112821,   3837,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  55806,  22243,  33071,  74040,   1773,  63109, 101408, 100439,\n",
            "          42140,  17714,  17881,  99178,  33071,  63109, 101408, 100439,   3837,\n",
            "          63109, 101408,  99877,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  63109, 101408, 105753, 117908,   5373,  74040,  33071,   5373,\n",
            "         100250,  99561,   3837,  63109,  67279, 100439,  33071, 101595,  99842,\n",
            "         100636, 100347, 102914,  20221,  33071,  63109,  67279, 100439,  49567,\n",
            "           1773, 109861, 100771, 101364, 100439,   5373, 119386, 101002, 100439,\n",
            "           5373, 101335, 110776, 100439,   5373, 102512, 100439,   5373, 101743,\n",
            "         120449, 100439,   3837, 100636,  99873,  81217, 119386,  99825, 102150,\n",
            "         100771,     40,  24300, 107218,   3837,  87267,  57218, 101320,   9370,\n",
            "         101041, 106722],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [101232, 104204, 103118,  52853,   3837,  29524, 101631, 100517, 108735,\n",
            "           5373,  99180,  35551, 109167, 101232, 101913, 101538,  99676,  49567,\n",
            "          24968, 101034, 101979, 114194,   3837,  29524,  99389,  89481,  99931,\n",
            "         114194,  33108, 100521,  99665, 114194,  49567,   8997,     17,     13,\n",
            "         116711, 100154,  23990,  71137,  89481, 116711, 102150,  72448, 100630,\n",
            "         102713, 101047,  82894,  99314,  26288,  23990,  71137, 102150,   3837,\n",
            "         101364,   5373, 101696,   5373, 114814,  36885,   5373, 100049, 103661,\n",
            "          15946,  99549,  99996, 116711, 102150,  33108, 100646, 101425, 102150,\n",
            "           9909, 104033,  15946,  33071, 101425, 102150,  74276, 104017,  71268,\n",
            "         100629,  65676,  65278, 112880, 116711,  98380,   3837,  30440, 107727,\n",
            "         111936, 102595,  99252,  52129,  31914,   8997,     18,  31914, 100058,\n",
            "         111841, 100630, 114672,  31914, 100058, 101047,  99622,  31914,   5373,\n",
            "         101631, 100517, 108735,      7,    398,    704,  20773,     68,  21280,\n",
            "            225, 101538,  54926, 102835,    955,    579,   2248,    439,    258,\n",
            "          21280,    225]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 4.4106, 'grad_norm': 2.40234637260437, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
            "{'loss': 3.8393, 'grad_norm': 2.7381951808929443, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
            "{'loss': 3.8409, 'grad_norm': 2.283769130706787, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
            "{'loss': 3.7873, 'grad_norm': 2.582184314727783, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
            "{'loss': 3.5772, 'grad_norm': 2.6323821544647217, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
            "{'loss': 3.8956, 'grad_norm': 3.112778902053833, 'learning_rate': 0.000197979797979798, 'epoch': 0.06}\n",
            "  6% 50/834 [00:32<08:25,  1.55it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 10.20it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 2.7461485862731934, 'eval_accuracy': 0.4409448818897638, 'eval_runtime': 0.7777, 'eval_samples_per_second': 12.859, 'eval_steps_per_second': 5.144, 'epoch': 0.06}\n",
            "  6% 50/834 [00:33<08:25,  1.55it/s]\n",
            "100% 4/4 [00:00<00:00,  7.16it/s]\u001b[A\n",
            "{'loss': 3.6475, 'grad_norm': 2.591639518737793, 'learning_rate': 0.00019545454545454548, 'epoch': 0.07}\n",
            "{'loss': 3.6665, 'grad_norm': 3.0626792907714844, 'learning_rate': 0.00019292929292929293, 'epoch': 0.08}\n",
            "{'loss': 3.6371, 'grad_norm': 2.6979610919952393, 'learning_rate': 0.0001904040404040404, 'epoch': 0.1}\n",
            "{'loss': 3.6115, 'grad_norm': 2.830892562866211, 'learning_rate': 0.0001878787878787879, 'epoch': 0.11}\n",
            "{'loss': 3.6603, 'grad_norm': 2.567694664001465, 'learning_rate': 0.00018535353535353537, 'epoch': 0.12}\n",
            " 12% 100/834 [01:06<08:10,  1.50it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.45it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.707350254058838, 'eval_accuracy': 0.4456692913385827, 'eval_runtime': 0.768, 'eval_samples_per_second': 13.021, 'eval_steps_per_second': 5.208, 'epoch': 0.12}\n",
            " 12% 100/834 [01:07<08:10,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.7209, 'grad_norm': 2.4075350761413574, 'learning_rate': 0.00018282828282828283, 'epoch': 0.13}\n",
            "{'loss': 3.3593, 'grad_norm': 2.726762294769287, 'learning_rate': 0.0001803030303030303, 'epoch': 0.14}\n",
            "{'loss': 3.6127, 'grad_norm': 2.64705753326416, 'learning_rate': 0.00017777777777777779, 'epoch': 0.16}\n",
            "{'loss': 3.687, 'grad_norm': 2.903027057647705, 'learning_rate': 0.00017525252525252527, 'epoch': 0.17}\n",
            "{'loss': 3.6439, 'grad_norm': 2.467358350753784, 'learning_rate': 0.00017272727272727275, 'epoch': 0.18}\n",
            " 18% 150/834 [01:41<07:51,  1.45it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.692863941192627, 'eval_accuracy': 0.4448818897637795, 'eval_runtime': 0.7856, 'eval_samples_per_second': 12.729, 'eval_steps_per_second': 5.091, 'epoch': 0.18}\n",
            " 18% 150/834 [01:42<07:51,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.5087, 'grad_norm': 2.615102529525757, 'learning_rate': 0.0001702020202020202, 'epoch': 0.19}\n",
            "{'loss': 3.4508, 'grad_norm': 2.8900229930877686, 'learning_rate': 0.00016767676767676768, 'epoch': 0.2}\n",
            "{'loss': 3.5026, 'grad_norm': 2.890944242477417, 'learning_rate': 0.00016515151515151516, 'epoch': 0.22}\n",
            "{'loss': 3.2993, 'grad_norm': 2.3885231018066406, 'learning_rate': 0.00016262626262626264, 'epoch': 0.23}\n",
            "{'loss': 3.5911, 'grad_norm': 2.2975189685821533, 'learning_rate': 0.00016010101010101012, 'epoch': 0.24}\n",
            " 24% 200/834 [02:17<07:06,  1.49it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.689362049102783, 'eval_accuracy': 0.4456692913385827, 'eval_runtime': 0.7761, 'eval_samples_per_second': 12.884, 'eval_steps_per_second': 5.154, 'epoch': 0.24}\n",
            " 24% 200/834 [02:17<07:06,  1.49it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.437, 'grad_norm': 2.5273044109344482, 'learning_rate': 0.00015757575757575757, 'epoch': 0.25}\n",
            "{'loss': 3.3178, 'grad_norm': 2.4527721405029297, 'learning_rate': 0.00015505050505050508, 'epoch': 0.26}\n",
            "{'loss': 3.5112, 'grad_norm': 2.591953992843628, 'learning_rate': 0.00015252525252525253, 'epoch': 0.28}\n",
            "{'loss': 3.3608, 'grad_norm': 2.3856968879699707, 'learning_rate': 0.00015000000000000001, 'epoch': 0.29}\n",
            "{'loss': 3.5516, 'grad_norm': 2.375535488128662, 'learning_rate': 0.00014747474747474747, 'epoch': 0.3}\n",
            " 30% 250/834 [02:52<06:36,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.647263526916504, 'eval_accuracy': 0.44881889763779526, 'eval_runtime': 0.7801, 'eval_samples_per_second': 12.818, 'eval_steps_per_second': 5.127, 'epoch': 0.3}\n",
            " 30% 250/834 [02:53<06:36,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.4017, 'grad_norm': 2.3003957271575928, 'learning_rate': 0.00014494949494949495, 'epoch': 0.31}\n",
            "{'loss': 3.4457, 'grad_norm': 2.399409770965576, 'learning_rate': 0.00014242424242424243, 'epoch': 0.32}\n",
            "{'loss': 3.3351, 'grad_norm': 2.9294419288635254, 'learning_rate': 0.0001398989898989899, 'epoch': 0.34}\n",
            "{'loss': 3.2997, 'grad_norm': 2.4120900630950928, 'learning_rate': 0.0001373737373737374, 'epoch': 0.35}\n",
            "{'loss': 3.4794, 'grad_norm': 2.7163712978363037, 'learning_rate': 0.00013484848484848484, 'epoch': 0.36}\n",
            " 36% 300/834 [03:27<06:04,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6050689220428467, 'eval_accuracy': 0.4574803149606299, 'eval_runtime': 0.773, 'eval_samples_per_second': 12.937, 'eval_steps_per_second': 5.175, 'epoch': 0.36}\n",
            " 36% 300/834 [03:28<06:04,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.4465, 'grad_norm': 2.3502840995788574, 'learning_rate': 0.00013232323232323235, 'epoch': 0.37}\n",
            "{'loss': 3.5152, 'grad_norm': 2.597365617752075, 'learning_rate': 0.0001297979797979798, 'epoch': 0.38}\n",
            "{'loss': 3.4158, 'grad_norm': 2.625171184539795, 'learning_rate': 0.00012727272727272728, 'epoch': 0.4}\n",
            "{'loss': 3.3884, 'grad_norm': 2.4753823280334473, 'learning_rate': 0.00012474747474747473, 'epoch': 0.41}\n",
            "{'loss': 3.4701, 'grad_norm': 2.4613397121429443, 'learning_rate': 0.00012222222222222224, 'epoch': 0.42}\n",
            " 42% 350/834 [04:02<05:28,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.30it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6125950813293457, 'eval_accuracy': 0.4559055118110236, 'eval_runtime': 0.7795, 'eval_samples_per_second': 12.829, 'eval_steps_per_second': 5.132, 'epoch': 0.42}\n",
            " 42% 350/834 [04:03<05:28,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.401, 'grad_norm': 2.41758131980896, 'learning_rate': 0.00011969696969696971, 'epoch': 0.43}\n",
            "{'loss': 3.3637, 'grad_norm': 2.511866569519043, 'learning_rate': 0.00011717171717171717, 'epoch': 0.44}\n",
            "{'loss': 3.4147, 'grad_norm': 2.8052284717559814, 'learning_rate': 0.00011464646464646464, 'epoch': 0.46}\n",
            "{'loss': 3.3527, 'grad_norm': 2.4974091053009033, 'learning_rate': 0.00011212121212121212, 'epoch': 0.47}\n",
            "{'loss': 3.5146, 'grad_norm': 2.9059269428253174, 'learning_rate': 0.0001095959595959596, 'epoch': 0.48}\n",
            " 48% 400/834 [04:37<04:53,  1.48it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.604918956756592, 'eval_accuracy': 0.4590551181102362, 'eval_runtime': 0.7791, 'eval_samples_per_second': 12.835, 'eval_steps_per_second': 5.134, 'epoch': 0.48}\n",
            " 48% 400/834 [04:38<04:53,  1.48it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.5611, 'grad_norm': 2.6163809299468994, 'learning_rate': 0.00010707070707070708, 'epoch': 0.49}\n",
            "{'loss': 3.5162, 'grad_norm': 2.5656135082244873, 'learning_rate': 0.00010454545454545455, 'epoch': 0.5}\n",
            "{'loss': 3.3404, 'grad_norm': 2.5903234481811523, 'learning_rate': 0.00010202020202020202, 'epoch': 0.52}\n",
            "{'loss': 3.2628, 'grad_norm': 2.4890847206115723, 'learning_rate': 9.94949494949495e-05, 'epoch': 0.53}\n",
            "{'loss': 3.6013, 'grad_norm': 2.6101760864257812, 'learning_rate': 9.696969696969698e-05, 'epoch': 0.54}\n",
            " 54% 450/834 [05:12<04:20,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5894551277160645, 'eval_accuracy': 0.4637795275590551, 'eval_runtime': 0.781, 'eval_samples_per_second': 12.804, 'eval_steps_per_second': 5.121, 'epoch': 0.54}\n",
            " 54% 450/834 [05:13<04:20,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.95it/s]\u001b[A\n",
            "{'loss': 3.4385, 'grad_norm': 2.7752246856689453, 'learning_rate': 9.444444444444444e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4278, 'grad_norm': 2.4279184341430664, 'learning_rate': 9.191919191919192e-05, 'epoch': 0.56}\n",
            "{'loss': 3.479, 'grad_norm': 2.6643600463867188, 'learning_rate': 8.93939393939394e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4674, 'grad_norm': 2.534480571746826, 'learning_rate': 8.686868686868688e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4069, 'grad_norm': 2.4395837783813477, 'learning_rate': 8.434343434343435e-05, 'epoch': 0.6}\n",
            " 60% 500/834 [05:48<03:45,  1.48it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.28it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.568115472793579, 'eval_accuracy': 0.462992125984252, 'eval_runtime': 0.7834, 'eval_samples_per_second': 12.765, 'eval_steps_per_second': 5.106, 'epoch': 0.6}\n",
            " 60% 500/834 [05:48<03:45,  1.48it/s]\n",
            "100% 4/4 [00:00<00:00,  6.88it/s]\u001b[A\n",
            "{'loss': 3.3861, 'grad_norm': 2.4794020652770996, 'learning_rate': 8.181818181818183e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3195, 'grad_norm': 2.2243094444274902, 'learning_rate': 7.92929292929293e-05, 'epoch': 0.62}\n",
            "{'loss': 3.5653, 'grad_norm': 2.344104290008545, 'learning_rate': 7.676767676767676e-05, 'epoch': 0.64}\n",
            "{'loss': 3.6057, 'grad_norm': 2.807245969772339, 'learning_rate': 7.424242424242424e-05, 'epoch': 0.65}\n",
            "{'loss': 3.4732, 'grad_norm': 2.330740213394165, 'learning_rate': 7.171717171717171e-05, 'epoch': 0.66}\n",
            " 66% 550/834 [06:23<03:13,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.99it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.41it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5615670680999756, 'eval_accuracy': 0.4653543307086614, 'eval_runtime': 0.7728, 'eval_samples_per_second': 12.939, 'eval_steps_per_second': 5.176, 'epoch': 0.66}\n",
            " 66% 550/834 [06:24<03:13,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.99it/s]\u001b[A\n",
            "{'loss': 3.3246, 'grad_norm': 2.460824966430664, 'learning_rate': 6.91919191919192e-05, 'epoch': 0.67}\n",
            "{'loss': 3.3566, 'grad_norm': 2.477255344390869, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.68}\n",
            "{'loss': 3.5534, 'grad_norm': 2.4287421703338623, 'learning_rate': 6.414141414141415e-05, 'epoch': 0.7}\n",
            "{'loss': 3.2224, 'grad_norm': 2.4379162788391113, 'learning_rate': 6.161616161616162e-05, 'epoch': 0.71}\n",
            "{'loss': 3.2942, 'grad_norm': 2.8559210300445557, 'learning_rate': 5.90909090909091e-05, 'epoch': 0.72}\n",
            " 72% 600/834 [06:58<02:39,  1.46it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.08it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.546313762664795, 'eval_accuracy': 0.46614173228346456, 'eval_runtime': 0.7712, 'eval_samples_per_second': 12.967, 'eval_steps_per_second': 5.187, 'epoch': 0.72}\n",
            " 72% 600/834 [06:59<02:39,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.2893, 'grad_norm': 2.527845621109009, 'learning_rate': 5.6565656565656563e-05, 'epoch': 0.73}\n",
            "{'loss': 3.2902, 'grad_norm': 2.882049322128296, 'learning_rate': 5.4040404040404044e-05, 'epoch': 0.74}\n",
            "{'loss': 3.3263, 'grad_norm': 2.200112819671631, 'learning_rate': 5.151515151515152e-05, 'epoch': 0.76}\n",
            "{'loss': 3.2348, 'grad_norm': 2.371873617172241, 'learning_rate': 4.898989898989899e-05, 'epoch': 0.77}\n",
            "{'loss': 3.2793, 'grad_norm': 2.417447566986084, 'learning_rate': 4.6464646464646464e-05, 'epoch': 0.78}\n",
            " 78% 650/834 [07:33<02:04,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5363147258758545, 'eval_accuracy': 0.46771653543307085, 'eval_runtime': 0.777, 'eval_samples_per_second': 12.871, 'eval_steps_per_second': 5.148, 'epoch': 0.78}\n",
            " 78% 650/834 [07:34<02:04,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.4664, 'grad_norm': 2.7108044624328613, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.79}\n",
            "{'loss': 3.3671, 'grad_norm': 2.8162765502929688, 'learning_rate': 4.141414141414142e-05, 'epoch': 0.8}\n",
            "{'loss': 3.4017, 'grad_norm': 2.4421255588531494, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.82}\n",
            "{'loss': 3.3557, 'grad_norm': 2.2384049892425537, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.83}\n",
            "{'loss': 3.299, 'grad_norm': 2.2656517028808594, 'learning_rate': 3.3838383838383844e-05, 'epoch': 0.84}\n",
            " 84% 700/834 [08:08<01:31,  1.46it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.02it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5439085960388184, 'eval_accuracy': 0.4716535433070866, 'eval_runtime': 0.7753, 'eval_samples_per_second': 12.898, 'eval_steps_per_second': 5.159, 'epoch': 0.84}\n",
            " 84% 700/834 [08:09<01:31,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.337, 'grad_norm': 2.3830652236938477, 'learning_rate': 3.131313131313132e-05, 'epoch': 0.85}\n",
            "{'loss': 3.4918, 'grad_norm': 2.7832751274108887, 'learning_rate': 2.878787878787879e-05, 'epoch': 0.86}\n",
            "{'loss': 3.3415, 'grad_norm': 2.5675175189971924, 'learning_rate': 2.6262626262626268e-05, 'epoch': 0.88}\n",
            "{'loss': 3.3237, 'grad_norm': 2.457533359527588, 'learning_rate': 2.3737373737373738e-05, 'epoch': 0.89}\n",
            "{'loss': 3.5257, 'grad_norm': 2.713085412979126, 'learning_rate': 2.1212121212121215e-05, 'epoch': 0.9}\n",
            " 90% 750/834 [08:43<00:57,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.537853717803955, 'eval_accuracy': 0.47244094488188976, 'eval_runtime': 0.7756, 'eval_samples_per_second': 12.893, 'eval_steps_per_second': 5.157, 'epoch': 0.9}\n",
            " 90% 750/834 [08:44<00:57,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.3977, 'grad_norm': 2.3714938163757324, 'learning_rate': 1.8686868686868688e-05, 'epoch': 0.91}\n",
            "{'loss': 3.4042, 'grad_norm': 2.6808242797851562, 'learning_rate': 1.6161616161616165e-05, 'epoch': 0.92}\n",
            "{'loss': 3.1036, 'grad_norm': 2.8067383766174316, 'learning_rate': 1.3636363636363637e-05, 'epoch': 0.94}\n",
            "{'loss': 3.4714, 'grad_norm': 2.6947989463806152, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.95}\n",
            "{'loss': 3.4058, 'grad_norm': 2.530618190765381, 'learning_rate': 8.585858585858587e-06, 'epoch': 0.96}\n",
            " 96% 800/834 [09:19<00:23,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.03it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5346555709838867, 'eval_accuracy': 0.4732283464566929, 'eval_runtime': 0.7711, 'eval_samples_per_second': 12.968, 'eval_steps_per_second': 5.187, 'epoch': 0.96}\n",
            " 96% 800/834 [09:19<00:23,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  7.01it/s]\u001b[A\n",
            "{'loss': 3.4833, 'grad_norm': 2.201910972595215, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.97}\n",
            "{'loss': 3.3677, 'grad_norm': 2.5260608196258545, 'learning_rate': 3.5353535353535352e-06, 'epoch': 0.98}\n",
            "{'loss': 3.4288, 'grad_norm': 2.581355333328247, 'learning_rate': 1.0101010101010103e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 583.6689, 'train_samples_per_second': 4.287, 'train_steps_per_second': 1.429, 'train_loss': 3.4570771675887437, 'epoch': 1.0}\n",
            "100% 834/834 [09:43<00:00,  1.43it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648356GF\n",
            "  train_loss               =     3.4571\n",
            "  train_runtime            = 0:09:43.66\n",
            "  train_samples            =       2502\n",
            "  train_samples_per_second =      4.287\n",
            "  train_steps_per_second   =      1.429\n",
            "\u001b[32m2025-12-16 09:33:01.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 583.6689, 'train_samples_per_second': 4.287, 'train_steps_per_second': 1.429, 'total_flos': 696167143243776.0, 'train_loss': 3.4570771675887437, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
            "\u001b[32m2025-12-16 09:33:01.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m734\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-12-16 09:33:01.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m742\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "100% 4/4 [00:00<00:00,  7.07it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.4748\n",
            "  eval_loss               =     2.5337\n",
            "  eval_runtime            = 0:00:00.77\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     12.917\n",
            "  eval_steps_per_second   =      5.167\n",
            "  perplexity              =    12.5996\n",
            "\u001b[32m2025-12-16 09:33:02.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m755\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 2.533663749694824, 'eval_accuracy': 0.4748031496062992, 'eval_runtime': 0.7742, 'eval_samples_per_second': 12.917, 'eval_steps_per_second': 5.167, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 12.599583397178508}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eaJl3pX2RHxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c33f613-f973-42dd-89fc-7b206c0c19b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  792 Dec 16 09:33 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 16 09:33 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 16 09:33 added_tokens.json\n",
            "-rw-r--r-- 1 root root  471 Dec 16 09:33 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:32 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:32 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:33 \u001b[01;34mcheckpoint-834\u001b[0m/\n",
            "-rw-r--r-- 1 root root  262 Dec 16 09:33 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 09:33 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Dec 16 09:33 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 16 09:23 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Dec 16 09:33 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.2K Dec 16 09:33 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Dec 16 09:33 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 16 09:33 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 16 09:33 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C7JkIeRHxO"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IMr9a1vCRHxO"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gqEX8d-HRHxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4722794d-4d44-4d06-c672-c53267bb7943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:33:39.066198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765877619.087411    6526 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765877619.093933    6526 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765877619.110175    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110204    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110208    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110212    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Loading LoRA for causal language model\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-pt/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XnkFdZAVRHxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b0c225-89a6-43c2-97f0-a6c76ef7ee3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 16 09:33 added_tokens.json\n",
            "-rw-r--r-- 1 root root  745 Dec 16 09:33 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 16 09:33 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 09:33 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 16 09:33 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 16 09:33 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 16 09:33 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 16 09:33 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 16 09:33 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6xx_eabWRHxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf156ba5-0347-4fe0-a588-19b8c7559ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1awjsMRHxP"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "tR18yzPBRHxP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5wY_nUKPRHxP"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gZcStwFLRHxP"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OvF3kATcRHxQ"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "jlFQomBpRHxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e714c8eb-bece-4572-a1aa-104fd3d37e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl        sharegpt_zh_1K_format.jsonl\n",
            "numina_cot_sharegpt_data_1k.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/content/MedicalGPT/data/finetune\")\n",
        "\n",
        "for path in data_dir.glob(\"*.jsonl\"):\n",
        "    new_lines = []\n",
        "    removed = 0\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            if \"id\" in obj:\n",
        "                obj.pop(\"id\")\n",
        "                removed += 1\n",
        "            new_lines.append(obj)\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for obj in new_lines:\n",
        "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ {path.name}: removed id from {removed} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfRriNWEhKgo",
        "outputId": "f5223b3b-308a-43ae-cb5f-8462557ae282"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ medical_sft_1K_format.jsonl: removed id from 0 samples\n",
            "✅ sharegpt_zh_1K_format.jsonl: removed id from 0 samples\n",
            "✅ numina_cot_sharegpt_data_1k.jsonl: removed id from 0 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "niwvj5S-RHxQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7d39fd-72b0-4b37-d323-016e64766078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:38:00.941095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765877880.960803    7651 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765877880.966896    7651 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765877880.982703    7651 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877880.982726    7651 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877880.982729    7651 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877880.982733    7651 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-16 09:38:06.083\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m348\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Dec16_09-38-06_913ef8db9873,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-sft-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mtrain files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1meval files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl']\u001b[0m\n",
            "Generating train split: 3000 examples [00:00, 78671.23 examples/s]\n",
            "Generating validation split: 3000 examples [00:00, 92938.95 examples/s]\n",
            "\u001b[32m2025-12-16 09:38:06.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:06.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m534\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': \"Define a new operation $\\\\#$ such that $a \\\\# b = (a+b)^3(a-b)$. What is the value of $8 \\\\# 2$? Let's think step by step and output the final answer within \\\\boxed{}.\"}, {'from': 'gpt', 'value': 'Plugging in to the new definition of the operation, we calculate $8 \\\\# 2$:\\n1. Calculate $a+b$: $8+2 = 10$.\\n2. Calculate $a-b$: $8-2 = 6$.\\n3. Compute $(a+b)^3$: $10^3 = 1000$.\\n4. Multiply $(a+b)^3$ by $(a-b)$: $1000 \\\\cdot 6 = 6000$.\\n\\nTherefore, the value of $8 \\\\# 2$ is $\\\\boxed{6000}$.'}]}\u001b[0m\n",
            "Running tokenizer on dataset: 100% 1000/1000 [00:10<00:00, 97.48 examples/s]\n",
            "Filter: 100% 995/995 [00:00<00:00, 2430.62 examples/s]\n",
            "\u001b[32m2025-12-16 09:38:20.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m551\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 995\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:20.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:20.088\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m553\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: Define a new operation $\\#$ such that $a \\# b = (a+b)^3(a-b)$. What is the value of $8 \\# 2$? Let's think step by step and output the final answer within \\boxed{}. ASSISTANT:Plugging in to the new definition of the operation, we calculate $8 \\# 2$:\n",
            "1. Calculate $a+b$: $8+2 = 10$.\n",
            "2. Calculate $a-b$: $8-2 = 6$.\n",
            "3. Compute $(a+b)^3$: $10^3 = 1000$.\n",
            "4. Multiply $(a+b)^3$ by $(a-b)$: $1000 \\cdot 6 = 6000$.\n",
            "\n",
            "Therefore, the value of $8 \\# 2$ is $\\boxed{6000}$.<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:20.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Plugging in to the new definition of the operation, we calculate $8 \\# 2$:\n",
            "1. Calculate $a+b$: $8+2 = 10$.\n",
            "2. Calculate $a-b$: $8-2 = 6$.\n",
            "3. Compute $(a+b)^3$: $10^3 = 1000$.\n",
            "4. Multiply $(a+b)^3$ by $(a-b)$: $1000 \\cdot 6 = 6000$.\n",
            "\n",
            "Therefore, the value of $8 \\# 2$ is $\\boxed{6000}$.<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:20.091\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:20.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？'}, {'from': 'gpt', 'value': '男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。'}]}\u001b[0m\n",
            "Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 138.61 examples/s]\n",
            "Filter: 100% 10/10 [00:00<00:00, 2023.11 examples/s]\n",
            "\u001b[32m2025-12-16 09:38:22.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:22.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:22.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m586\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？ ASSISTANT:男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:22.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1m🔧 大模型训练配置:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:22.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1m  model_kwargs: {'config': Qwen2Config {\n",
            "  \"_name_or_path\": \"merged-pt\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            ", 'torch_dtype': torch.bfloat16, 'trust_remote_code': True, 'quantization_config': None, 'low_cpu_mem_usage': True, 'device_map': 'auto'}\u001b[0m\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m713\u001b[0m - \u001b[1m✅ 模型加载完成\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[1m📊 模型分布情况:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m718\u001b[0m - \u001b[1m🔧 使用HuggingFace设备映射:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m720\u001b[0m - \u001b[1m  : 0\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m728\u001b[0m - \u001b[1m📈 设备使用统计:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m730\u001b[0m - \u001b[1m  0: 1 个模块\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m751\u001b[0m - \u001b[1m💾 GPU内存使用情况:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m756\u001b[0m - \u001b[1m  GPU 0: 已分配=0.9GB, 缓存=1.0GB, 总计=14.7GB\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m798\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m813\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m822\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:23.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m823\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "\u001b[32m2025-12-16 09:38:24.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m845\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
            "/content/MedicalGPT/supervised_finetuning.py:862: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-12-16 09:38:24.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:24.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m876\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    32,   6236,   1948,  ...,     13, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,     13, 151643,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100]],\n",
            "       device='cuda:0')}\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:24.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m877\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
            "[tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,   2585,   1657,   6785,   3429,\n",
            "         41214,    653,    220,     24,     17,     19,     15,    323,    220,\n",
            "            16,     15,     23,     15,     15,    614,    304,   4185,     30,\n",
            "          6771,    594,   1744,   3019,    553,   3019,    323,   2550,    279,\n",
            "          1590,   4226,   2878,   1124,  79075,  46391,  35560,   3846,   2821,\n",
            "            25,   5338,     11,  11047,    279,  44858,    315,    220,     24,\n",
            "            17,     19,     15,    323,    220,     16,     15,     23,     15,\n",
            "            15,    382,     16,     13,  37729,    551,   2176,   5109,    510,\n",
            "           256,    481,    220,     24,     17,     19,     15,    284,    400,\n",
            "            17,     61,     18,   1124,  50853,    220,     18,   1124,  50853,\n",
            "           220,     20,   1124,  50853,    220,     22,   1124,  50853,    220,\n",
            "            16,     16,  25046,    256,    481,    220,     16,     15,     23,\n",
            "            15,     15,    284,    400,     17,     61,     18,   1124,  50853,\n",
            "           220,     18,     61,     18,   1124,  50853,    220,     20,     61,\n",
            "            17,  66426,     17,     13,  11778,    279,   8028,   2355,    315,\n",
            "          4185,  10250,   9363,    510,    256,    481,  44858,      7,     24,\n",
            "            17,     19,     15,     11,    220,     16,     15,     23,     15,\n",
            "            15,      8,    284,    400,     17,     61,     18,   1124,  50853,\n",
            "           220,     18,     61,     16,   1124,  50853,    220,     20,     61,\n",
            "            16,  66426,     18,     13,    576,  44858,    374,    400,     17,\n",
            "            61,     18,   1124,  50853,    220,     18,   1124,  50853,    220,\n",
            "            20,    284,    220,     16,     17,     15,      3,    624,    256,\n",
            "           481,    220,     16,     17,     15,    284,    400,     17,     61,\n",
            "            18,   1124,  50853,    220,     18,     61,     16,   1124,  50853,\n",
            "           220,     20,     61,     16,  66426,     19,     13,   4504,    279,\n",
            "          3429,  41214,    315,    220,     16,     17,     15,    510,    256,\n",
            "           481,  10657,   3429,  41214,    284,   4930,     18,     10,     16,\n",
            "          2376,     16,     10,     16,   2376,     16,     10,     16,      8,\n",
            "           284,    220,     19,   1124,  15136,    220,     17,   1124,  15136,\n",
            "           220,     17,    284,    220,     16,     21,  66426,  54815,     11,\n",
            "           220,     24,     17,     19,     15,    323,    220,     16,     15,\n",
            "            23,     15,     15,    614,  57960,  79075,     90,     16,     21,\n",
            "         31716,   6785,   3429,  41214,    304,   4185,     13, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,   6567,    224,     96, 109689,\n",
            "        114814, 102150,  99243,  99389,  99252,  18493, 117528, 101072,  26232,\n",
            "         99405, 107000, 101037,   3837,  99811, 109689, 114814, 102150,  99243,\n",
            "         99389,  99252,  18493, 117528, 101072,  26232,  99405, 107000, 101037,\n",
            "            30,  35560,   3846,   2821,     25, 106141, 101042,   5122, 111308,\n",
            "          3837, 107000,  73670, 107645,   3837, 104361, 102100,   1773, 110290,\n",
            "          5122,  73670,  99405, 107000,   3837, 105745,  99564,   1773, 101885,\n",
            "        117528, 101072, 116021,  36993,  99572,   3837,  30534,  42140, 104361,\n",
            "        102100,   1773, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,    220,  99285, 112210,  99389,\n",
            "         99769,   9370, 104595, 101107, 102021, 104256,  11319,   3837,  35946,\n",
            "        112607, 101099, 108684,   1773, 100681, 104037, 105788, 101270,   1773,\n",
            "        106428,  99916, 105562, 109913,   1773, 102863, 100762,  20412,  70927,\n",
            "        106408,  22226,  52801,   3837, 100131,  20450, 112514, 100000,  99654,\n",
            "          1773, 100346,  99285, 112210,  99389,  99769,   9370, 107837, 101037,\n",
            "          1773, 106922, 105907,     30,  99285, 112210,  99389,  99769,   9370,\n",
            "        107837, 102021,  11319,  35560,   3846,   2821,     25, 108386,   1773,\n",
            "         23031, 115346, 105640, 105200, 110403,   3837, 121003, 113788, 100327,\n",
            "        115483,  57191, 102505,   1773,  39165, 102357, 101408,  99204, 100426,\n",
            "         13343,  46448,  30440, 113914, 102357, 103985,   1773, 116124, 118439,\n",
            "        107837, 101368,  17714, 100150, 103677,   5373, 118087, 117258,   5373,\n",
            "         99315,  77128,  99773, 101735,   5373, 101738,  99450,   5373,  99315,\n",
            "         77128, 106122,   5373, 100636, 102997,  99589,  49567,   1773,  99285,\n",
            "        112210,  99389,  99769, 101924, 101421, 102890,  72448, 104110, 103989,\n",
            "        116178,   3837, 100000,  31235, 111874, 104196,  18947,  86119,   1773,\n",
            "         99285,  99389, 112210,  13343, 100141,  17714,  63109, 101408, 105487,\n",
            "         33071, 100649,   3837,  30440, 103989, 100347,  63109, 103255,   5373,\n",
            "         63109,  99499,  20726,  38953,   1773, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0')], \n",
            "labels:\n",
            "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   5338,     11,  11047,    279,  44858,    315,    220,     24,\n",
            "            17,     19,     15,    323,    220,     16,     15,     23,     15,\n",
            "            15,    382,     16,     13,  37729,    551,   2176,   5109,    510,\n",
            "           256,    481,    220,     24,     17,     19,     15,    284,    400,\n",
            "            17,     61,     18,   1124,  50853,    220,     18,   1124,  50853,\n",
            "           220,     20,   1124,  50853,    220,     22,   1124,  50853,    220,\n",
            "            16,     16,  25046,    256,    481,    220,     16,     15,     23,\n",
            "            15,     15,    284,    400,     17,     61,     18,   1124,  50853,\n",
            "           220,     18,     61,     18,   1124,  50853,    220,     20,     61,\n",
            "            17,  66426,     17,     13,  11778,    279,   8028,   2355,    315,\n",
            "          4185,  10250,   9363,    510,    256,    481,  44858,      7,     24,\n",
            "            17,     19,     15,     11,    220,     16,     15,     23,     15,\n",
            "            15,      8,    284,    400,     17,     61,     18,   1124,  50853,\n",
            "           220,     18,     61,     16,   1124,  50853,    220,     20,     61,\n",
            "            16,  66426,     18,     13,    576,  44858,    374,    400,     17,\n",
            "            61,     18,   1124,  50853,    220,     18,   1124,  50853,    220,\n",
            "            20,    284,    220,     16,     17,     15,      3,    624,    256,\n",
            "           481,    220,     16,     17,     15,    284,    400,     17,     61,\n",
            "            18,   1124,  50853,    220,     18,     61,     16,   1124,  50853,\n",
            "           220,     20,     61,     16,  66426,     19,     13,   4504,    279,\n",
            "          3429,  41214,    315,    220,     16,     17,     15,    510,    256,\n",
            "           481,  10657,   3429,  41214,    284,   4930,     18,     10,     16,\n",
            "          2376,     16,     10,     16,   2376,     16,     10,     16,      8,\n",
            "           284,    220,     19,   1124,  15136,    220,     17,   1124,  15136,\n",
            "           220,     17,    284,    220,     16,     21,  66426,  54815,     11,\n",
            "           220,     24,     17,     19,     15,    323,    220,     16,     15,\n",
            "            23,     15,     15,    614,  57960,  79075,     90,     16,     21,\n",
            "         31716,   6785,   3429,  41214,    304,   4185,     13, 151643,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100, 106141, 101042,   5122, 111308,\n",
            "          3837, 107000,  73670, 107645,   3837, 104361, 102100,   1773, 110290,\n",
            "          5122,  73670,  99405, 107000,   3837, 105745,  99564,   1773, 101885,\n",
            "        117528, 101072, 116021,  36993,  99572,   3837,  30534,  42140, 104361,\n",
            "        102100,   1773, 151643,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100, 108386,   1773,\n",
            "         23031, 115346, 105640, 105200, 110403,   3837, 121003, 113788, 100327,\n",
            "        115483,  57191, 102505,   1773,  39165, 102357, 101408,  99204, 100426,\n",
            "         13343,  46448,  30440, 113914, 102357, 103985,   1773, 116124, 118439,\n",
            "        107837, 101368,  17714, 100150, 103677,   5373, 118087, 117258,   5373,\n",
            "         99315,  77128,  99773, 101735,   5373, 101738,  99450,   5373,  99315,\n",
            "         77128, 106122,   5373, 100636, 102997,  99589,  49567,   1773,  99285,\n",
            "        112210,  99389,  99769, 101924, 101421, 102890,  72448, 104110, 103989,\n",
            "        116178,   3837, 100000,  31235, 111874, 104196,  18947,  86119,   1773,\n",
            "         99285,  99389, 112210,  13343, 100141,  17714,  63109, 101408, 105487,\n",
            "         33071, 100649,   3837,  30440, 103989, 100347,  63109, 103255,   5373,\n",
            "         63109,  99499,  20726,  38953,   1773, 151643,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0')]\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:24.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m878\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: How many positive divisors do 9240 and 10800 have in common? Let's think step by step and output the final answer within \\boxed{}. ASSISTANT:First, calculate the gcd of 9240 and 10800.\n",
            "\n",
            "1. Factorize both numbers:\n",
            "   - 9240 = $2^3 \\cdot 3 \\cdot 5 \\cdot 7 \\cdot 11$\n",
            "   - 10800 = $2^3 \\cdot 3^3 \\cdot 5^2$\n",
            "\n",
            "2. Take the minimum power of common prime factors:\n",
            "   - gcd(9240, 10800) = $2^3 \\cdot 3^1 \\cdot 5^1$\n",
            "\n",
            "3. The gcd is $2^3 \\cdot 3 \\cdot 5 = 120$.\n",
            "   - 120 = $2^3 \\cdot 3^1 \\cdot 5^1$\n",
            "\n",
            "4. Count the divisors of 120:\n",
            "   - Total divisors = $(3+1)(1+1)(1+1) = 4 \\times 2 \\times 2 = 16$\n",
            "\n",
            "Therefore, 9240 and 10800 have $\\boxed{16}$ positive divisors in common.<|endoftext|><|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-16 09:38:24.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m881\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>First, calculate the gcd of 9240 and 10800.\n",
            "\n",
            "1. Factorize both numbers:\n",
            "   - 9240 = $2^3 \\cdot 3 \\cdot 5 \\cdot 7 \\cdot 11$\n",
            "   - 10800 = $2^3 \\cdot 3^3 \\cdot 5^2$\n",
            "\n",
            "2. Take the minimum power of common prime factors:\n",
            "   - gcd(9240, 10800) = $2^3 \\cdot 3^1 \\cdot 5^1$\n",
            "\n",
            "3. The gcd is $2^3 \\cdot 3 \\cdot 5 = 120$.\n",
            "   - 120 = $2^3 \\cdot 3^1 \\cdot 5^1$\n",
            "\n",
            "4. Count the divisors of 120:\n",
            "   - Total divisors = $(3+1)(1+1)(1+1) = 4 \\times 2 \\times 2 = 16$\n",
            "\n",
            "Therefore, 9240 and 10800 have $\\boxed{16}$ positive divisors in common.<|endoftext|><|endoftext|>\u001b[0m\n",
            "{'loss': 2.066, 'grad_norm': 1.6229028701782227, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
            "{'loss': 1.41, 'grad_norm': 1.0702069997787476, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
            "{'loss': 1.6313, 'grad_norm': 1.3332946300506592, 'learning_rate': 1.940677966101695e-05, 'epoch': 0.08}\n",
            "{'loss': 1.6557, 'grad_norm': 0.8616651892662048, 'learning_rate': 1.8559322033898307e-05, 'epoch': 0.12}\n",
            "{'loss': 2.1514, 'grad_norm': 1.0530569553375244, 'learning_rate': 1.7711864406779662e-05, 'epoch': 0.16}\n",
            "{'loss': 1.5738, 'grad_norm': 1.7869987487792969, 'learning_rate': 1.6864406779661018e-05, 'epoch': 0.2}\n",
            " 20% 50/249 [02:25<09:51,  2.97s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.61it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 3.2035107612609863, 'eval_runtime': 2.1061, 'eval_samples_per_second': 4.748, 'eval_steps_per_second': 1.424, 'epoch': 0.2}\n",
            " 20% 50/249 [02:28<09:51,  2.97s/it]\n",
            "100% 3/3 [00:01<00:00,  1.78it/s]\u001b[A\n",
            "{'loss': 2.0264, 'grad_norm': 1.134769082069397, 'learning_rate': 1.6016949152542373e-05, 'epoch': 0.24}\n",
            "{'loss': 1.9368, 'grad_norm': 2.57711124420166, 'learning_rate': 1.5169491525423729e-05, 'epoch': 0.28}\n",
            "{'loss': 1.5679, 'grad_norm': 3.6266489028930664, 'learning_rate': 1.4322033898305086e-05, 'epoch': 0.32}\n",
            "{'loss': 1.5287, 'grad_norm': 0.9901688694953918, 'learning_rate': 1.3474576271186442e-05, 'epoch': 0.36}\n",
            "{'loss': 1.511, 'grad_norm': 1.510705590248108, 'learning_rate': 1.2627118644067797e-05, 'epoch': 0.4}\n",
            " 40% 100/249 [04:57<07:17,  2.94s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1400489807128906, 'eval_runtime': 2.1235, 'eval_samples_per_second': 4.709, 'eval_steps_per_second': 1.413, 'epoch': 0.4}\n",
            " 40% 100/249 [04:59<07:17,  2.94s/it]\n",
            "100% 3/3 [00:01<00:00,  1.77it/s]\u001b[A\n",
            "{'loss': 1.3572, 'grad_norm': 1.1284761428833008, 'learning_rate': 1.1779661016949153e-05, 'epoch': 0.44}\n",
            "{'loss': 1.7134, 'grad_norm': 1.1854959726333618, 'learning_rate': 1.0932203389830509e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5808, 'grad_norm': 1.4408748149871826, 'learning_rate': 1.0084745762711864e-05, 'epoch': 0.52}\n",
            "{'loss': 1.5813, 'grad_norm': 1.8823983669281006, 'learning_rate': 9.237288135593222e-06, 'epoch': 0.56}\n",
            "{'loss': 1.7276, 'grad_norm': 0.8614572882652283, 'learning_rate': 8.389830508474577e-06, 'epoch': 0.6}\n",
            " 60% 150/249 [07:30<04:47,  2.91s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1135172843933105, 'eval_runtime': 2.1108, 'eval_samples_per_second': 4.738, 'eval_steps_per_second': 1.421, 'epoch': 0.6}\n",
            " 60% 150/249 [07:32<04:47,  2.91s/it]\n",
            "100% 3/3 [00:01<00:00,  1.78it/s]\u001b[A\n",
            "{'loss': 1.4584, 'grad_norm': 1.0433580875396729, 'learning_rate': 7.542372881355933e-06, 'epoch': 0.64}\n",
            "{'loss': 1.5371, 'grad_norm': 0.9701420068740845, 'learning_rate': 6.694915254237288e-06, 'epoch': 0.68}\n",
            "{'loss': 1.6579, 'grad_norm': 2.3650028705596924, 'learning_rate': 5.847457627118645e-06, 'epoch': 0.72}\n",
            "{'loss': 1.627, 'grad_norm': 0.9865198135375977, 'learning_rate': 5e-06, 'epoch': 0.76}\n",
            "{'loss': 1.4714, 'grad_norm': 1.1462113857269287, 'learning_rate': 4.152542372881356e-06, 'epoch': 0.8}\n",
            " 80% 200/249 [10:01<02:28,  3.04s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.59it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.105316638946533, 'eval_runtime': 2.1081, 'eval_samples_per_second': 4.744, 'eval_steps_per_second': 1.423, 'epoch': 0.8}\n",
            " 80% 200/249 [10:03<02:28,  3.04s/it]\n",
            "100% 3/3 [00:01<00:00,  1.78it/s]\u001b[A\n",
            "{'loss': 1.5821, 'grad_norm': 1.1101635694503784, 'learning_rate': 3.305084745762712e-06, 'epoch': 0.84}\n",
            "{'loss': 1.4135, 'grad_norm': 1.5825296640396118, 'learning_rate': 2.457627118644068e-06, 'epoch': 0.88}\n",
            "{'loss': 1.6896, 'grad_norm': 1.5999572277069092, 'learning_rate': 1.6101694915254237e-06, 'epoch': 0.92}\n",
            "{'loss': 1.8159, 'grad_norm': 1.504189372062683, 'learning_rate': 7.627118644067798e-07, 'epoch': 0.96}\n",
            "{'train_runtime': 747.3394, 'train_samples_per_second': 1.331, 'train_steps_per_second': 0.333, 'train_loss': 1.6316904399289665, 'epoch': 1.0}\n",
            "100% 249/249 [12:27<00:00,  3.00s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   959480GF\n",
            "  train_loss               =     1.6317\n",
            "  train_runtime            = 0:12:27.33\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      1.331\n",
            "  train_steps_per_second   =      0.333\n",
            "\u001b[32m2025-12-16 09:50:52.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m898\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 747.3394, 'train_samples_per_second': 1.331, 'train_steps_per_second': 0.333, 'total_flos': 1030234160173056.0, 'train_loss': 1.6316904399289665, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
            "\u001b[32m2025-12-16 09:50:52.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m899\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
            "\u001b[32m2025-12-16 09:50:52.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m908\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 3/3 [00:01<00:00,  1.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =      3.101\n",
            "  eval_runtime            = 0:00:02.11\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      4.738\n",
            "  eval_steps_per_second   =      1.422\n",
            "  perplexity              =    22.2201\n",
            "\u001b[32m2025-12-16 09:50:54.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m921\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.1009981632232666, 'eval_runtime': 2.1104, 'eval_samples_per_second': 4.738, 'eval_steps_per_second': 1.422, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 22.220119521963266}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T2nHmAYyRHxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3daf882-e099-4ec2-e675-eb833fb85f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  784 Dec 16 09:50 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 16 09:50 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 16 09:50 added_tokens.json\n",
            "-rw-r--r-- 1 root root  431 Dec 16 09:50 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:50 \u001b[0m\u001b[01;34mcheckpoint-249\u001b[0m/\n",
            "-rw-r--r-- 1 root root  221 Dec 16 09:50 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 09:50 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Dec 16 09:50 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 16 09:38 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Dec 16 09:50 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.2K Dec 16 09:50 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 6.1K Dec 16 09:50 trainer_state.json\n",
            "-rw-r--r-- 1 root root  230 Dec 16 09:50 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 16 09:50 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sKk0B_RwRHxR"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "K6zQAzonRHxR"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zQFzf_9lRHxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc3e1e7-2628-4aa8-cf34-60b54fb70d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 10:09:37.876155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765879777.895736   15729 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765879777.901702   15729 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765879777.916763   15729 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765879777.916791   15729 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765879777.916795   15729 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765879777.916800   15729 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Namespace(base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-pt\n",
            "LoRA model: outputs-sft-v1\n",
            "Loading LoRA for causal language model\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to ./merged-sft\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qDOhmlfxRHxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fcfd74-65b0-475d-8074-2e151f448374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 16 10:09 added_tokens.json\n",
            "-rw-r--r-- 1 root root  737 Dec 16 10:09 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 16 10:09 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 10:09 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 16 10:09 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 16 10:09 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 16 10:09 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 16 10:09 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 16 10:09 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j7wNjXbcRHxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7523f9ee-d2f8-44e1-bcaa-99f4de18eeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"_name_or_path\": \"merged-pt\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aiWF8j4JRHxR"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "OiEpzDyARHxR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ByqcBIWLRHxR"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yTj8WbcBRHxR"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Zsei_JC7RHxS"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SaFl3W7wRHxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2deb3e-2c65-43ab-b624-fd6a4e7d3069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trl==0.9.6"
      ],
      "metadata": {
        "id": "0XGl-Zy9p7EA",
        "outputId": "6472c55f-f9c1-4ef4-ce7b-8c0cb82aa397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl==0.9.6\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (4.49.0)\n",
            "Collecting numpy<2.0.0,>=1.18.2 (from trl==0.9.6)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (1.12.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (4.0.0)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
            "  Downloading tyro-1.0.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.9.6) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.31.0->trl==0.9.6) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n",
            "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.2-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.6/180.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tyro, trl\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.26.1\n",
            "    Uninstalling trl-0.26.1:\n",
            "      Successfully uninstalled trl-0.26.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 trl-0.9.6 tyro-1.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "trl"
                ]
              },
              "id": "be19a98a823d40f2b277f35a940a590b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Q__SOEl-RHxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94667cf0-ddf5-48e5-bfe8-cdeb5989dc29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 10:15:07.481310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765880107.500726   17138 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765880107.506598   17138 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765880107.521616   17138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765880107.521642   17138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765880107.521646   17138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765880107.521649   17138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-16 10:15:14.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mParse args: ScriptArguments(model_name_or_path='./merged-sft', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', template_name='qwen', per_device_train_batch_size=3, per_device_eval_batch_size=1, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=500, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, peft_path=None, do_train=True, do_eval=True, learning_rate=0.0005, lr_scheduler_type='cosine', warmup_steps=100, weight_decay=0.05, optim='adamw_hf', fp16=False, bf16=True, gradient_checkpointing=True, gradient_accumulation_steps=4, save_steps=50, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-v1', max_steps=100, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.218\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='./merged-sft', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "Generating train split: 500 examples [00:00, 32628.82 examples/s]\n",
            "Generating validation split: 500 examples [00:00, 73386.01 examples/s]\n",
            "\u001b[32m2025-12-16 10:15:15.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.492\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m321\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "Running tokenizer on dataset (num_proc=4): 100% 500/500 [00:00<00:00, 1446.87 examples/s]\n",
            "Filter: 100% 500/500 [00:00<00:00, 48332.61 examples/s]\n",
            "\u001b[32m2025-12-16 10:15:15.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m334\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 160\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m335\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m337\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "\n",
            "<|im_start|>user\n",
            "耶稣学院，牛津大学是何时建立的，建立它的灵感是什么？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m338\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "耶稣学院（Jesus College）是由伦敦富有的商人和公务员托马斯·怀特爵士（Sir Thomas White）于1571年创立的，他通过在纺织品贸易中的商业活动积累了可观的财富。学院建立的初衷是创建一个致力于学术、团结和知识追求的社区。其主要目标之一是为有才华的个人提供教育和机会，不论他们的背景或社会地位如何。基于这一考虑，学院设立了大量基金来支持学者和学生，鼓励他们申请并在各个知识领域进行学习。如今，耶稣学院（Jesus College）仍致力于学术卓越，鼓励学生探索各种学科，促进学院内外的包容和支持性社区。\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "耶稣学院是由伊丽莎白一世的顾问威廉·塞西尔爵士于1571年创立的。建立这所学院的灵感来自于为准备进入英国教会事奉的学生提供神学和希腊语教育。学院以耶稣基督命名，其使命是推广基督教神学和美德。\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:15.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m351\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "Running tokenizer on dataset (num_proc=4): 100% 500/500 [00:00<00:00, 1804.30 examples/s]\n",
            "Filter: 100% 500/500 [00:00<00:00, 53531.55 examples/s]\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 160\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "你是一个非常聪明的AI助手，非常擅长按照指示行事。尽你所能地帮助。\n",
            "<|im_start|>user\n",
            "期末考试 问题1. 罗伯特·P·凯利曾任CEO的公司是在哪一年成立的？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "为了帮助您解答这个问题，我需要知道罗伯特·P·凯利曾担任过的公司的名称。请提供公司名称。\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "当然！罗伯特·P·凯利曾担任首席执行官的公司Uber成立于2009年3月28日。\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:16.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mDevice map: auto\u001b[0m\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[32m2025-12-16 10:15:17.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-16 10:15:17.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m449\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/dpo_training.py\", line 501, in <module>\n",
            "    main()\n",
            "  File \"/content/MedicalGPT/dpo_training.py\", line 460, in main\n",
            "    trainer = DPOTrainer(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "TypeError: DPOTrainer.__init__() got an unexpected keyword argument 'processing_class'\n"
          ]
        }
      ],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMh1QfZiRHxS"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "AjUBaPuNRHxS"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1ve7ePqhRHxS"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGjwlFcoRHxS"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsDiNzjcRHxT"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM3RQqxTRHxT"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qrUmGHAsRHxT"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Prwrq_ciRHxT"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "9eAyVOUcRHxT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pee2Vq50RHxU"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "vX5f_R__RHxU"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kZwu9mH1RHxU"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uqu_TyrRHxU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}