{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CkandT/Colab/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hCXK5o17RHxJ"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cL8hy5W3RHxK"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XJbav5RHxL"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f8pDlFsDRHxL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZULztBcRHxL"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vpzDxbiYRHxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bd60e8-c1a7-42f6-882d-ef7297982ccf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 98 (delta 19), reused 52 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 8.98 MiB | 11.28 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/MedicalGPT/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.7.3)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.49.0)\n",
            "Requirement already satisfied: trl>=0.15.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.15.2)\n",
            "Requirement already satisfied: latex2sympy2_extended in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (1.0.6)\n",
            "Requirement already satisfied: math-verify==0.5.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.5.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.21.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl>=0.15.2->-r requirements.txt (line 10)) (13.9.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl>=0.15.2->-r requirements.txt (line 10)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl>=0.15.2->-r requirements.txt (line 10)) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl>=0.15.2->-r requirements.txt (line 10)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5df53U6b6pj",
        "outputId": "0723c067-c7e9-455d-d522-d9158280f5d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.14.0\n",
            "✅ Transformers版本: 4.49.0\n",
            "✅ TRL版本: 0.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.49.0 peft==0.14.0 trl==0.15.2 accelerate==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP9JyUKOdKpP",
        "outputId": "94f84f92-7a98-480f-ed12-026bf38c98bb",
        "collapsed": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.49.0 in /usr/local/lib/python3.12/dist-packages (4.49.0)\n",
            "Requirement already satisfied: peft==0.14.0 in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: trl==0.15.2 in /usr/local/lib/python3.12/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate==1.4.0 in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.15.2) (4.0.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.15.2) (13.9.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->trl==0.15.2) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->trl==0.15.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->trl==0.15.2) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->trl==0.15.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->trl==0.15.2) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.15.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.15.2) (2.19.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (3.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.15.2) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.2) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl==0.15.2) (1.17.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 249, in _attempt_to_pin_criterion\n",
            "    satisfied = all(\n",
            "                ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 250, in <genexpr>\n",
            "    self._p.is_satisfied_by(requirement=r, candidate=candidate)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 375, in __hash__\n",
            "    return hash((self.name, self.version))\n",
            "                            ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 388, in version\n",
            "    self._version = self.dist.version\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 175, in version\n",
            "    return parse_version(self._dist.version)\n",
            "                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 467, in version\n",
            "    return self.metadata['Version']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 452, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 64, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 53, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 174, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 178, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 462, in _parsegen\n",
            "    for line in self._input:\n",
            "                ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 127, in __next__\n",
            "    def __next__(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 1703, in print\n",
            "    extend(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/segment.py\", line 198, in <genexpr>\n",
            "    cls(text, None if control else apply(_style), control)\n",
            "                                   ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/style.py\", line 758, in __add__\n",
            "    combined_style = self._add(style)\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/style.py\", line 732, in _add\n",
            "    @lru_cache(maxsize=1024)\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCbPPkptdcr2",
        "outputId": "c8e106b4-d3ce-4ecc-e1d1-080a14ffa826"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.14.0\n",
            "✅ Transformers版本: 4.49.0\n",
            "✅ TRL版本: 0.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from transformers import is_torch_tpu_available; import peft; import trl; print('环境验证通过！')\""
      ],
      "metadata": {
        "id": "nx2xe0NjkwPd",
        "outputId": "2a70982a-6cec-4d2f-d0eb-ccb018f0af6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/peft/__init__.py\", line 22, in <module>\n",
            "    from .auto import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/peft/auto.py\", line 21, in <module>\n",
            "    from transformers import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1852, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n",
            "    from .auto_factory import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
            "    from ...generation import GenerationMixin\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1851, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1863, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 53, in <module>\n",
            "    from .candidate_generator import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/candidate_generator.py\", line 26, in <module>\n",
            "    from sklearn.metrics import roc_curve\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\", line 73, in <module>\n",
            "    from .base import clone  # noqa: E402\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 19, in <module>\n",
            "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
            "    from ._chunking import gen_batches, gen_even_slices\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
            "    from ._param_validation import Interval, validate_params\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
            "    from .validation import _is_arraylike_not_scalar\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
            "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
            "    from .fixes import parse_version\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/api.py\", line 47, in <module>\n",
            "    from pandas.core.groupby import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
            "    from pandas.core.groupby.generic import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/generic.py\", line 68, in <module>\n",
            "    from pandas.core.frame import DataFrame\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 188, in <module>\n",
            "    from pandas.core.series import Series\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\", line 263, in <module>\n",
            "    class Series(base.IndexOpsMixin, NDFrame):  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\", line 5125, in Series\n",
            "    @Appender(NDFrame.set_axis.__doc__)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\", line 488, in __call__\n",
            "    func.__doc__ = dedent(self.join.join(docitems))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/textwrap.py\", line 443, in dedent\n",
            "    elif indent.startswith(margin):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_KL15Aowdswo",
        "outputId": "320d18e5-dda3-49cd-9b97-e14f71f9b131"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/MedicalGPT/MedicalGPT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/MedicalGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYC6lC9Ld4nJ",
        "outputId": "9de69337-9a6d-4b0b-a458-13f72109802a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MedicalGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo1r9sL3RHxM"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "I8EhYMc7RHxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbe8887-4cde-4c6a-baff-6de56676c9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gy_iFL9RHxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2a9ba2e6-4382-4a7c-90a8-0105002233bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-18 06:22:39.799203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766038959.819441   25541 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766038959.825497   25541 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766038959.842693   25541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766038959.842720   25541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766038959.842724   25541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766038959.842728   25541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-18 06:22:45.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Dec18_06-22-44_bd24742b1e5a,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:45.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:46.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.165\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
            "limit to the factories, forges, refineries, and railways that could be\n",
            "built, to the multitudes that could be employed in conquering a\n",
            "continent. As for the future, that was in the hands of Providence!\n",
            "\n",
            "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
            "and the planters of Calhoun's had their theories of government and\n",
            "politics, so the leaders in business enterprise had theirs. It was\n",
            "simple and easily stated. \"It is the duty of the government,\" they\n",
            "ur\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:52.168\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
            "limit to the factories, forges, refineries, and railways that could be\n",
            "built, to the multitudes that could be employed in conquering a\n",
            "continent. As for the future, that was in the hands of Providence!\n",
            "\n",
            "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
            "and the planters of Calhoun's had their theories of government and\n",
            "politics, so the leaders in business enterprise had theirs. It was\n",
            "simple and easily stated. \"It is the duty of the government,\" they\n",
            "ur\u001b[0m\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[32m2025-12-18 06:22:53.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:53.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:53.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:53.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py:658: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-12-18 06:22:53.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-18 06:22:54.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
            "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
            "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
            "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
            "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
            "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
            "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
            "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
            "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
            "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
            "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
            "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
            "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
            "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
            "          21515,  57191],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
            "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
            "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
            "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
            "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
            "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
            "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
            "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
            "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
            "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
            "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
            "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
            "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
            "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
            "           1707,    304]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
            "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
            "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
            "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
            "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
            "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
            "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
            "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
            "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
            "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
            "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
            "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
            "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
            "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
            "          21515,  57191],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
            "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
            "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
            "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
            "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
            "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
            "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
            "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
            "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
            "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
            "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
            "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
            "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
            "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
            "           1707,    304]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 4.4106, 'grad_norm': 2.40234637260437, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
            "  1% 6/834 [00:04<08:58,  1.54it/s]"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaJl3pX2RHxO"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C7JkIeRHxO"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IMr9a1vCRHxO"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqEX8d-HRHxP"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnkFdZAVRHxP"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xx_eabWRHxP"
      },
      "outputs": [],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1awjsMRHxP"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "tR18yzPBRHxP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5wY_nUKPRHxP"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gZcStwFLRHxP"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OvF3kATcRHxQ"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "jlFQomBpRHxQ"
      },
      "outputs": [],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/content/MedicalGPT/data/finetune\")\n",
        "\n",
        "for path in data_dir.glob(\"*.jsonl\"):\n",
        "    new_lines = []\n",
        "    removed = 0\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            if \"id\" in obj:\n",
        "                obj.pop(\"id\")\n",
        "                removed += 1\n",
        "            new_lines.append(obj)\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for obj in new_lines:\n",
        "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ {path.name}: removed id from {removed} samples\")\n"
      ],
      "metadata": {
        "id": "vfRriNWEhKgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niwvj5S-RHxQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2nHmAYyRHxQ"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sKk0B_RwRHxR"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "K6zQAzonRHxR"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQFzf_9lRHxR"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOhmlfxRHxR"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7wNjXbcRHxR"
      },
      "outputs": [],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aiWF8j4JRHxR"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "OiEpzDyARHxR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ByqcBIWLRHxR"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yTj8WbcBRHxR"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Zsei_JC7RHxS"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaFl3W7wRHxS"
      },
      "outputs": [],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trl==0.9.6"
      ],
      "metadata": {
        "id": "0XGl-Zy9p7EA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q__SOEl-RHxS"
      },
      "outputs": [],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMh1QfZiRHxS"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "AjUBaPuNRHxS"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1ve7ePqhRHxS"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGjwlFcoRHxS"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsDiNzjcRHxT"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM3RQqxTRHxT"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qrUmGHAsRHxT"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Prwrq_ciRHxT"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "9eAyVOUcRHxT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pee2Vq50RHxU"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "vX5f_R__RHxU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kZwu9mH1RHxU"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uqu_TyrRHxU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}