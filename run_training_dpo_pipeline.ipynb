{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CkandT/Colab/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hCXK5o17RHxJ"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cL8hy5W3RHxK"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XJbav5RHxL"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f8pDlFsDRHxL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZULztBcRHxL"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vpzDxbiYRHxM",
        "outputId": "fd15ad1d-7945-4f2d-e346-8291a5ba3a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 98 (delta 19), reused 52 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 8.98 MiB | 23.63 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Collecting loguru (from -r requirements.txt (line 3))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.3)\n",
            "Collecting trl>=0.15.2 (from -r requirements.txt (line 10))\n",
            "  Downloading trl-0.26.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2 (from -r requirements.txt (line 13))\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended->-r requirements.txt (line 12))\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.26.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime, loguru, latex2sympy2_extended, math-verify, trl\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.0.6 loguru-0.7.3 math-verify-0.5.2 trl-0.26.1\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "id": "u5df53U6b6pj",
        "outputId": "338d601b-b33d-4304-edbd-5cbebcc489b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.18.0\n",
            "✅ Transformers版本: 4.57.3\n",
            "✅ TRL版本: 0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.49.0 peft==0.14.0"
      ],
      "metadata": {
        "id": "UP9JyUKOdKpP",
        "outputId": "db62e47a-220f-4fbc-bf17-1f60615a0519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.49.0\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.14.0\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.14.0) (1.12.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (3.0.3)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trl 0.26.1 requires transformers>=4.56.1, but you have transformers 4.49.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.14.0 tokenizers-0.21.4 transformers-4.49.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "peft",
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "7de54562e29148adaaf30c0f328bc777"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft, transformers, trl\n",
        "print(f'✅ PEFT版本: {peft.__version__}')\n",
        "print(f'✅ Transformers版本: {transformers.__version__}')\n",
        "print(f'✅ TRL版本: {trl.__version__}')"
      ],
      "metadata": {
        "id": "pCbPPkptdcr2",
        "outputId": "8f8a7d07-e813-446c-ae71-273009cbf3d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PEFT版本: 0.14.0\n",
            "✅ Transformers版本: 4.49.0\n",
            "✅ TRL版本: 0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "_KL15Aowdswo",
        "outputId": "51335e03-5d05-4839-c991-dfd061d426c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/MedicalGPT"
      ],
      "metadata": {
        "id": "pYC6lC9Ld4nJ",
        "outputId": "3938f08b-4f21-432b-da59-c94e7ea98425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MedicalGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo1r9sL3RHxM"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I8EhYMc7RHxN",
        "outputId": "a1fefa3c-7e47-4ca0-8968-cdfea219106f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1gy_iFL9RHxN",
        "outputId": "44dcfc94-c0a7-4e06-f149-6677a0fe3fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:22:33.628705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765876953.648313    3655 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765876953.654346    3655 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765876953.669526    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669551    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669555    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765876953.669560    3655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-16 09:22:39.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Dec16_09-22-38_913ef8db9873,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:39.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 7.23kB [00:00, 30.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 71.3MB/s]\n",
            "merges.txt: 1.67MB [00:00, 145MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 184MB/s]\n",
            "\u001b[32m2025-12-16 09:22:40.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:22:40.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 235480.78 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 412188.39 examples/s]\n",
            "\u001b[32m2025-12-16 09:22:40.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 364.41 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 371.55 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9738.12 examples/s] \n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9488.42 examples/s] \n",
            "\u001b[32m2025-12-16 09:23:07.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.893\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:07.894\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 5.36MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:06<00:00, 164MB/s]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 998kB/s]\n",
            "\u001b[32m2025-12-16 09:23:15.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:15.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py:658: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-12-16 09:23:16.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-16 09:23:16.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[100673, 109838, 116545, 105252,   1773, 116124, 118439, 112140,  42140,\n",
            "          23031,  99931, 101232, 100439, 100782,   3837,  99931, 100815,  99178,\n",
            "           5373,  99243,  99178,  33108,  99931,  99251,  30440,  99726,  74040,\n",
            "         105178, 112821,   3837,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  55806,  22243,  33071,  74040,   1773,  63109, 101408, 100439,\n",
            "          42140,  17714,  17881,  99178,  33071,  63109, 101408, 100439,   3837,\n",
            "          63109, 101408,  99877,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  63109, 101408, 105753, 117908,   5373,  74040,  33071,   5373,\n",
            "         100250,  99561,   3837,  63109,  67279, 100439,  33071, 101595,  99842,\n",
            "         100636, 100347, 102914,  20221,  33071,  63109,  67279, 100439,  49567,\n",
            "           1773, 109861, 100771, 101364, 100439,   5373, 119386, 101002, 100439,\n",
            "           5373, 101335, 110776, 100439,   5373, 102512, 100439,   5373, 101743,\n",
            "         120449, 100439,   3837, 100636,  99873,  81217, 119386,  99825, 102150,\n",
            "         100771,     40,  24300, 107218,   3837,  87267,  57218, 101320,   9370,\n",
            "         101041, 106722],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [101232, 104204, 103118,  52853,   3837,  29524, 101631, 100517, 108735,\n",
            "           5373,  99180,  35551, 109167, 101232, 101913, 101538,  99676,  49567,\n",
            "          24968, 101034, 101979, 114194,   3837,  29524,  99389,  89481,  99931,\n",
            "         114194,  33108, 100521,  99665, 114194,  49567,   8997,     17,     13,\n",
            "         116711, 100154,  23990,  71137,  89481, 116711, 102150,  72448, 100630,\n",
            "         102713, 101047,  82894,  99314,  26288,  23990,  71137, 102150,   3837,\n",
            "         101364,   5373, 101696,   5373, 114814,  36885,   5373, 100049, 103661,\n",
            "          15946,  99549,  99996, 116711, 102150,  33108, 100646, 101425, 102150,\n",
            "           9909, 104033,  15946,  33071, 101425, 102150,  74276, 104017,  71268,\n",
            "         100629,  65676,  65278, 112880, 116711,  98380,   3837,  30440, 107727,\n",
            "         111936, 102595,  99252,  52129,  31914,   8997,     18,  31914, 100058,\n",
            "         111841, 100630, 114672,  31914, 100058, 101047,  99622,  31914,   5373,\n",
            "         101631, 100517, 108735,      7,    398,    704,  20773,     68,  21280,\n",
            "            225, 101538,  54926, 102835,    955,    579,   2248,    439,    258,\n",
            "          21280,    225]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[100673, 109838, 116545, 105252,   1773, 116124, 118439, 112140,  42140,\n",
            "          23031,  99931, 101232, 100439, 100782,   3837,  99931, 100815,  99178,\n",
            "           5373,  99243,  99178,  33108,  99931,  99251,  30440,  99726,  74040,\n",
            "         105178, 112821,   3837,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  55806,  22243,  33071,  74040,   1773,  63109, 101408, 100439,\n",
            "          42140,  17714,  17881,  99178,  33071,  63109, 101408, 100439,   3837,\n",
            "          63109, 101408,  99877,  18830,  23990,  71137, 102150, 101595,  99842,\n",
            "          81217,  63109, 101408, 105753, 117908,   5373,  74040,  33071,   5373,\n",
            "         100250,  99561,   3837,  63109,  67279, 100439,  33071, 101595,  99842,\n",
            "         100636, 100347, 102914,  20221,  33071,  63109,  67279, 100439,  49567,\n",
            "           1773, 109861, 100771, 101364, 100439,   5373, 119386, 101002, 100439,\n",
            "           5373, 101335, 110776, 100439,   5373, 102512, 100439,   5373, 101743,\n",
            "         120449, 100439,   3837, 100636,  99873,  81217, 119386,  99825, 102150,\n",
            "         100771,     40,  24300, 107218,   3837,  87267,  57218, 101320,   9370,\n",
            "         101041, 106722],\n",
            "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
            "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
            "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
            "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
            "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
            "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
            "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
            "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
            "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
            "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
            "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
            "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
            "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
            "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
            "          75405, 106783],\n",
            "        [101232, 104204, 103118,  52853,   3837,  29524, 101631, 100517, 108735,\n",
            "           5373,  99180,  35551, 109167, 101232, 101913, 101538,  99676,  49567,\n",
            "          24968, 101034, 101979, 114194,   3837,  29524,  99389,  89481,  99931,\n",
            "         114194,  33108, 100521,  99665, 114194,  49567,   8997,     17,     13,\n",
            "         116711, 100154,  23990,  71137,  89481, 116711, 102150,  72448, 100630,\n",
            "         102713, 101047,  82894,  99314,  26288,  23990,  71137, 102150,   3837,\n",
            "         101364,   5373, 101696,   5373, 114814,  36885,   5373, 100049, 103661,\n",
            "          15946,  99549,  99996, 116711, 102150,  33108, 100646, 101425, 102150,\n",
            "           9909, 104033,  15946,  33071, 101425, 102150,  74276, 104017,  71268,\n",
            "         100629,  65676,  65278, 112880, 116711,  98380,   3837,  30440, 107727,\n",
            "         111936, 102595,  99252,  52129,  31914,   8997,     18,  31914, 100058,\n",
            "         111841, 100630, 114672,  31914, 100058, 101047,  99622,  31914,   5373,\n",
            "         101631, 100517, 108735,      7,    398,    704,  20773,     68,  21280,\n",
            "            225, 101538,  54926, 102835,    955,    579,   2248,    439,    258,\n",
            "          21280,    225]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 4.4106, 'grad_norm': 2.40234637260437, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
            "{'loss': 3.8393, 'grad_norm': 2.7381951808929443, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
            "{'loss': 3.8409, 'grad_norm': 2.283769130706787, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
            "{'loss': 3.7873, 'grad_norm': 2.582184314727783, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
            "{'loss': 3.5772, 'grad_norm': 2.6323821544647217, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
            "{'loss': 3.8956, 'grad_norm': 3.112778902053833, 'learning_rate': 0.000197979797979798, 'epoch': 0.06}\n",
            "  6% 50/834 [00:32<08:25,  1.55it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 10.20it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 2.7461485862731934, 'eval_accuracy': 0.4409448818897638, 'eval_runtime': 0.7777, 'eval_samples_per_second': 12.859, 'eval_steps_per_second': 5.144, 'epoch': 0.06}\n",
            "  6% 50/834 [00:33<08:25,  1.55it/s]\n",
            "100% 4/4 [00:00<00:00,  7.16it/s]\u001b[A\n",
            "{'loss': 3.6475, 'grad_norm': 2.591639518737793, 'learning_rate': 0.00019545454545454548, 'epoch': 0.07}\n",
            "{'loss': 3.6665, 'grad_norm': 3.0626792907714844, 'learning_rate': 0.00019292929292929293, 'epoch': 0.08}\n",
            "{'loss': 3.6371, 'grad_norm': 2.6979610919952393, 'learning_rate': 0.0001904040404040404, 'epoch': 0.1}\n",
            "{'loss': 3.6115, 'grad_norm': 2.830892562866211, 'learning_rate': 0.0001878787878787879, 'epoch': 0.11}\n",
            "{'loss': 3.6603, 'grad_norm': 2.567694664001465, 'learning_rate': 0.00018535353535353537, 'epoch': 0.12}\n",
            " 12% 100/834 [01:06<08:10,  1.50it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.45it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.707350254058838, 'eval_accuracy': 0.4456692913385827, 'eval_runtime': 0.768, 'eval_samples_per_second': 13.021, 'eval_steps_per_second': 5.208, 'epoch': 0.12}\n",
            " 12% 100/834 [01:07<08:10,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.7209, 'grad_norm': 2.4075350761413574, 'learning_rate': 0.00018282828282828283, 'epoch': 0.13}\n",
            "{'loss': 3.3593, 'grad_norm': 2.726762294769287, 'learning_rate': 0.0001803030303030303, 'epoch': 0.14}\n",
            "{'loss': 3.6127, 'grad_norm': 2.64705753326416, 'learning_rate': 0.00017777777777777779, 'epoch': 0.16}\n",
            "{'loss': 3.687, 'grad_norm': 2.903027057647705, 'learning_rate': 0.00017525252525252527, 'epoch': 0.17}\n",
            "{'loss': 3.6439, 'grad_norm': 2.467358350753784, 'learning_rate': 0.00017272727272727275, 'epoch': 0.18}\n",
            " 18% 150/834 [01:41<07:51,  1.45it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.692863941192627, 'eval_accuracy': 0.4448818897637795, 'eval_runtime': 0.7856, 'eval_samples_per_second': 12.729, 'eval_steps_per_second': 5.091, 'epoch': 0.18}\n",
            " 18% 150/834 [01:42<07:51,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.5087, 'grad_norm': 2.615102529525757, 'learning_rate': 0.0001702020202020202, 'epoch': 0.19}\n",
            "{'loss': 3.4508, 'grad_norm': 2.8900229930877686, 'learning_rate': 0.00016767676767676768, 'epoch': 0.2}\n",
            "{'loss': 3.5026, 'grad_norm': 2.890944242477417, 'learning_rate': 0.00016515151515151516, 'epoch': 0.22}\n",
            "{'loss': 3.2993, 'grad_norm': 2.3885231018066406, 'learning_rate': 0.00016262626262626264, 'epoch': 0.23}\n",
            "{'loss': 3.5911, 'grad_norm': 2.2975189685821533, 'learning_rate': 0.00016010101010101012, 'epoch': 0.24}\n",
            " 24% 200/834 [02:17<07:06,  1.49it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.689362049102783, 'eval_accuracy': 0.4456692913385827, 'eval_runtime': 0.7761, 'eval_samples_per_second': 12.884, 'eval_steps_per_second': 5.154, 'epoch': 0.24}\n",
            " 24% 200/834 [02:17<07:06,  1.49it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.437, 'grad_norm': 2.5273044109344482, 'learning_rate': 0.00015757575757575757, 'epoch': 0.25}\n",
            "{'loss': 3.3178, 'grad_norm': 2.4527721405029297, 'learning_rate': 0.00015505050505050508, 'epoch': 0.26}\n",
            "{'loss': 3.5112, 'grad_norm': 2.591953992843628, 'learning_rate': 0.00015252525252525253, 'epoch': 0.28}\n",
            "{'loss': 3.3608, 'grad_norm': 2.3856968879699707, 'learning_rate': 0.00015000000000000001, 'epoch': 0.29}\n",
            "{'loss': 3.5516, 'grad_norm': 2.375535488128662, 'learning_rate': 0.00014747474747474747, 'epoch': 0.3}\n",
            " 30% 250/834 [02:52<06:36,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.647263526916504, 'eval_accuracy': 0.44881889763779526, 'eval_runtime': 0.7801, 'eval_samples_per_second': 12.818, 'eval_steps_per_second': 5.127, 'epoch': 0.3}\n",
            " 30% 250/834 [02:53<06:36,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.4017, 'grad_norm': 2.3003957271575928, 'learning_rate': 0.00014494949494949495, 'epoch': 0.31}\n",
            "{'loss': 3.4457, 'grad_norm': 2.399409770965576, 'learning_rate': 0.00014242424242424243, 'epoch': 0.32}\n",
            "{'loss': 3.3351, 'grad_norm': 2.9294419288635254, 'learning_rate': 0.0001398989898989899, 'epoch': 0.34}\n",
            "{'loss': 3.2997, 'grad_norm': 2.4120900630950928, 'learning_rate': 0.0001373737373737374, 'epoch': 0.35}\n",
            "{'loss': 3.4794, 'grad_norm': 2.7163712978363037, 'learning_rate': 0.00013484848484848484, 'epoch': 0.36}\n",
            " 36% 300/834 [03:27<06:04,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6050689220428467, 'eval_accuracy': 0.4574803149606299, 'eval_runtime': 0.773, 'eval_samples_per_second': 12.937, 'eval_steps_per_second': 5.175, 'epoch': 0.36}\n",
            " 36% 300/834 [03:28<06:04,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.4465, 'grad_norm': 2.3502840995788574, 'learning_rate': 0.00013232323232323235, 'epoch': 0.37}\n",
            "{'loss': 3.5152, 'grad_norm': 2.597365617752075, 'learning_rate': 0.0001297979797979798, 'epoch': 0.38}\n",
            "{'loss': 3.4158, 'grad_norm': 2.625171184539795, 'learning_rate': 0.00012727272727272728, 'epoch': 0.4}\n",
            "{'loss': 3.3884, 'grad_norm': 2.4753823280334473, 'learning_rate': 0.00012474747474747473, 'epoch': 0.41}\n",
            "{'loss': 3.4701, 'grad_norm': 2.4613397121429443, 'learning_rate': 0.00012222222222222224, 'epoch': 0.42}\n",
            " 42% 350/834 [04:02<05:28,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.30it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6125950813293457, 'eval_accuracy': 0.4559055118110236, 'eval_runtime': 0.7795, 'eval_samples_per_second': 12.829, 'eval_steps_per_second': 5.132, 'epoch': 0.42}\n",
            " 42% 350/834 [04:03<05:28,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.401, 'grad_norm': 2.41758131980896, 'learning_rate': 0.00011969696969696971, 'epoch': 0.43}\n",
            "{'loss': 3.3637, 'grad_norm': 2.511866569519043, 'learning_rate': 0.00011717171717171717, 'epoch': 0.44}\n",
            "{'loss': 3.4147, 'grad_norm': 2.8052284717559814, 'learning_rate': 0.00011464646464646464, 'epoch': 0.46}\n",
            "{'loss': 3.3527, 'grad_norm': 2.4974091053009033, 'learning_rate': 0.00011212121212121212, 'epoch': 0.47}\n",
            "{'loss': 3.5146, 'grad_norm': 2.9059269428253174, 'learning_rate': 0.0001095959595959596, 'epoch': 0.48}\n",
            " 48% 400/834 [04:37<04:53,  1.48it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.604918956756592, 'eval_accuracy': 0.4590551181102362, 'eval_runtime': 0.7791, 'eval_samples_per_second': 12.835, 'eval_steps_per_second': 5.134, 'epoch': 0.48}\n",
            " 48% 400/834 [04:38<04:53,  1.48it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.5611, 'grad_norm': 2.6163809299468994, 'learning_rate': 0.00010707070707070708, 'epoch': 0.49}\n",
            "{'loss': 3.5162, 'grad_norm': 2.5656135082244873, 'learning_rate': 0.00010454545454545455, 'epoch': 0.5}\n",
            "{'loss': 3.3404, 'grad_norm': 2.5903234481811523, 'learning_rate': 0.00010202020202020202, 'epoch': 0.52}\n",
            "{'loss': 3.2628, 'grad_norm': 2.4890847206115723, 'learning_rate': 9.94949494949495e-05, 'epoch': 0.53}\n",
            "{'loss': 3.6013, 'grad_norm': 2.6101760864257812, 'learning_rate': 9.696969696969698e-05, 'epoch': 0.54}\n",
            " 54% 450/834 [05:12<04:20,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5894551277160645, 'eval_accuracy': 0.4637795275590551, 'eval_runtime': 0.781, 'eval_samples_per_second': 12.804, 'eval_steps_per_second': 5.121, 'epoch': 0.54}\n",
            " 54% 450/834 [05:13<04:20,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.95it/s]\u001b[A\n",
            "{'loss': 3.4385, 'grad_norm': 2.7752246856689453, 'learning_rate': 9.444444444444444e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4278, 'grad_norm': 2.4279184341430664, 'learning_rate': 9.191919191919192e-05, 'epoch': 0.56}\n",
            "{'loss': 3.479, 'grad_norm': 2.6643600463867188, 'learning_rate': 8.93939393939394e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4674, 'grad_norm': 2.534480571746826, 'learning_rate': 8.686868686868688e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4069, 'grad_norm': 2.4395837783813477, 'learning_rate': 8.434343434343435e-05, 'epoch': 0.6}\n",
            " 60% 500/834 [05:48<03:45,  1.48it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.28it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.568115472793579, 'eval_accuracy': 0.462992125984252, 'eval_runtime': 0.7834, 'eval_samples_per_second': 12.765, 'eval_steps_per_second': 5.106, 'epoch': 0.6}\n",
            " 60% 500/834 [05:48<03:45,  1.48it/s]\n",
            "100% 4/4 [00:00<00:00,  6.88it/s]\u001b[A\n",
            "{'loss': 3.3861, 'grad_norm': 2.4794020652770996, 'learning_rate': 8.181818181818183e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3195, 'grad_norm': 2.2243094444274902, 'learning_rate': 7.92929292929293e-05, 'epoch': 0.62}\n",
            "{'loss': 3.5653, 'grad_norm': 2.344104290008545, 'learning_rate': 7.676767676767676e-05, 'epoch': 0.64}\n",
            "{'loss': 3.6057, 'grad_norm': 2.807245969772339, 'learning_rate': 7.424242424242424e-05, 'epoch': 0.65}\n",
            "{'loss': 3.4732, 'grad_norm': 2.330740213394165, 'learning_rate': 7.171717171717171e-05, 'epoch': 0.66}\n",
            " 66% 550/834 [06:23<03:13,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.99it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.41it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5615670680999756, 'eval_accuracy': 0.4653543307086614, 'eval_runtime': 0.7728, 'eval_samples_per_second': 12.939, 'eval_steps_per_second': 5.176, 'epoch': 0.66}\n",
            " 66% 550/834 [06:24<03:13,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.99it/s]\u001b[A\n",
            "{'loss': 3.3246, 'grad_norm': 2.460824966430664, 'learning_rate': 6.91919191919192e-05, 'epoch': 0.67}\n",
            "{'loss': 3.3566, 'grad_norm': 2.477255344390869, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.68}\n",
            "{'loss': 3.5534, 'grad_norm': 2.4287421703338623, 'learning_rate': 6.414141414141415e-05, 'epoch': 0.7}\n",
            "{'loss': 3.2224, 'grad_norm': 2.4379162788391113, 'learning_rate': 6.161616161616162e-05, 'epoch': 0.71}\n",
            "{'loss': 3.2942, 'grad_norm': 2.8559210300445557, 'learning_rate': 5.90909090909091e-05, 'epoch': 0.72}\n",
            " 72% 600/834 [06:58<02:39,  1.46it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.08it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.546313762664795, 'eval_accuracy': 0.46614173228346456, 'eval_runtime': 0.7712, 'eval_samples_per_second': 12.967, 'eval_steps_per_second': 5.187, 'epoch': 0.72}\n",
            " 72% 600/834 [06:59<02:39,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.2893, 'grad_norm': 2.527845621109009, 'learning_rate': 5.6565656565656563e-05, 'epoch': 0.73}\n",
            "{'loss': 3.2902, 'grad_norm': 2.882049322128296, 'learning_rate': 5.4040404040404044e-05, 'epoch': 0.74}\n",
            "{'loss': 3.3263, 'grad_norm': 2.200112819671631, 'learning_rate': 5.151515151515152e-05, 'epoch': 0.76}\n",
            "{'loss': 3.2348, 'grad_norm': 2.371873617172241, 'learning_rate': 4.898989898989899e-05, 'epoch': 0.77}\n",
            "{'loss': 3.2793, 'grad_norm': 2.417447566986084, 'learning_rate': 4.6464646464646464e-05, 'epoch': 0.78}\n",
            " 78% 650/834 [07:33<02:04,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5363147258758545, 'eval_accuracy': 0.46771653543307085, 'eval_runtime': 0.777, 'eval_samples_per_second': 12.871, 'eval_steps_per_second': 5.148, 'epoch': 0.78}\n",
            " 78% 650/834 [07:34<02:04,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.4664, 'grad_norm': 2.7108044624328613, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.79}\n",
            "{'loss': 3.3671, 'grad_norm': 2.8162765502929688, 'learning_rate': 4.141414141414142e-05, 'epoch': 0.8}\n",
            "{'loss': 3.4017, 'grad_norm': 2.4421255588531494, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.82}\n",
            "{'loss': 3.3557, 'grad_norm': 2.2384049892425537, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.83}\n",
            "{'loss': 3.299, 'grad_norm': 2.2656517028808594, 'learning_rate': 3.3838383838383844e-05, 'epoch': 0.84}\n",
            " 84% 700/834 [08:08<01:31,  1.46it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.02it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5439085960388184, 'eval_accuracy': 0.4716535433070866, 'eval_runtime': 0.7753, 'eval_samples_per_second': 12.898, 'eval_steps_per_second': 5.159, 'epoch': 0.84}\n",
            " 84% 700/834 [08:09<01:31,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.337, 'grad_norm': 2.3830652236938477, 'learning_rate': 3.131313131313132e-05, 'epoch': 0.85}\n",
            "{'loss': 3.4918, 'grad_norm': 2.7832751274108887, 'learning_rate': 2.878787878787879e-05, 'epoch': 0.86}\n",
            "{'loss': 3.3415, 'grad_norm': 2.5675175189971924, 'learning_rate': 2.6262626262626268e-05, 'epoch': 0.88}\n",
            "{'loss': 3.3237, 'grad_norm': 2.457533359527588, 'learning_rate': 2.3737373737373738e-05, 'epoch': 0.89}\n",
            "{'loss': 3.5257, 'grad_norm': 2.713085412979126, 'learning_rate': 2.1212121212121215e-05, 'epoch': 0.9}\n",
            " 90% 750/834 [08:43<00:57,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.537853717803955, 'eval_accuracy': 0.47244094488188976, 'eval_runtime': 0.7756, 'eval_samples_per_second': 12.893, 'eval_steps_per_second': 5.157, 'epoch': 0.9}\n",
            " 90% 750/834 [08:44<00:57,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\u001b[A\n",
            "{'loss': 3.3977, 'grad_norm': 2.3714938163757324, 'learning_rate': 1.8686868686868688e-05, 'epoch': 0.91}\n",
            "{'loss': 3.4042, 'grad_norm': 2.6808242797851562, 'learning_rate': 1.6161616161616165e-05, 'epoch': 0.92}\n",
            "{'loss': 3.1036, 'grad_norm': 2.8067383766174316, 'learning_rate': 1.3636363636363637e-05, 'epoch': 0.94}\n",
            "{'loss': 3.4714, 'grad_norm': 2.6947989463806152, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.95}\n",
            "{'loss': 3.4058, 'grad_norm': 2.530618190765381, 'learning_rate': 8.585858585858587e-06, 'epoch': 0.96}\n",
            " 96% 800/834 [09:19<00:23,  1.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.03it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5346555709838867, 'eval_accuracy': 0.4732283464566929, 'eval_runtime': 0.7711, 'eval_samples_per_second': 12.968, 'eval_steps_per_second': 5.187, 'epoch': 0.96}\n",
            " 96% 800/834 [09:19<00:23,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  7.01it/s]\u001b[A\n",
            "{'loss': 3.4833, 'grad_norm': 2.201910972595215, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.97}\n",
            "{'loss': 3.3677, 'grad_norm': 2.5260608196258545, 'learning_rate': 3.5353535353535352e-06, 'epoch': 0.98}\n",
            "{'loss': 3.4288, 'grad_norm': 2.581355333328247, 'learning_rate': 1.0101010101010103e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 583.6689, 'train_samples_per_second': 4.287, 'train_steps_per_second': 1.429, 'train_loss': 3.4570771675887437, 'epoch': 1.0}\n",
            "100% 834/834 [09:43<00:00,  1.43it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648356GF\n",
            "  train_loss               =     3.4571\n",
            "  train_runtime            = 0:09:43.66\n",
            "  train_samples            =       2502\n",
            "  train_samples_per_second =      4.287\n",
            "  train_steps_per_second   =      1.429\n",
            "\u001b[32m2025-12-16 09:33:01.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 583.6689, 'train_samples_per_second': 4.287, 'train_steps_per_second': 1.429, 'total_flos': 696167143243776.0, 'train_loss': 3.4570771675887437, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
            "\u001b[32m2025-12-16 09:33:01.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m734\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-12-16 09:33:01.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m742\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "100% 4/4 [00:00<00:00,  7.07it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.4748\n",
            "  eval_loss               =     2.5337\n",
            "  eval_runtime            = 0:00:00.77\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     12.917\n",
            "  eval_steps_per_second   =      5.167\n",
            "  perplexity              =    12.5996\n",
            "\u001b[32m2025-12-16 09:33:02.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m755\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 2.533663749694824, 'eval_accuracy': 0.4748031496062992, 'eval_runtime': 0.7742, 'eval_samples_per_second': 12.917, 'eval_steps_per_second': 5.167, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 12.599583397178508}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eaJl3pX2RHxO",
        "outputId": "4c33f613-f973-42dd-89fc-7b206c0c19b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  792 Dec 16 09:33 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 16 09:33 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 16 09:33 added_tokens.json\n",
            "-rw-r--r-- 1 root root  471 Dec 16 09:33 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:32 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:32 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 16 09:33 \u001b[01;34mcheckpoint-834\u001b[0m/\n",
            "-rw-r--r-- 1 root root  262 Dec 16 09:33 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 09:33 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Dec 16 09:33 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 16 09:23 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Dec 16 09:33 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.2K Dec 16 09:33 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Dec 16 09:33 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 16 09:33 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 16 09:33 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C7JkIeRHxO"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IMr9a1vCRHxO"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gqEX8d-HRHxP",
        "outputId": "4722794d-4d44-4d06-c672-c53267bb7943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:33:39.066198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765877619.087411    6526 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765877619.093933    6526 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765877619.110175    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110204    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110208    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877619.110212    6526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Loading LoRA for causal language model\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-pt/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XnkFdZAVRHxP",
        "outputId": "28b0c225-89a6-43c2-97f0-a6c76ef7ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 16 09:33 added_tokens.json\n",
            "-rw-r--r-- 1 root root  745 Dec 16 09:33 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 16 09:33 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 16 09:33 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 16 09:33 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 16 09:33 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 16 09:33 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 16 09:33 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 16 09:33 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6xx_eabWRHxP",
        "outputId": "bf156ba5-0347-4fe0-a588-19b8c7559ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1awjsMRHxP"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "tR18yzPBRHxP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5wY_nUKPRHxP"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gZcStwFLRHxP"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OvF3kATcRHxQ"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "jlFQomBpRHxQ",
        "outputId": "e714c8eb-bece-4572-a1aa-104fd3d37e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl        sharegpt_zh_1K_format.jsonl\n",
            "numina_cot_sharegpt_data_1k.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/content/MedicalGPT/data/finetune\")\n",
        "\n",
        "for path in data_dir.glob(\"*.jsonl\"):\n",
        "    new_lines = []\n",
        "    removed = 0\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            if \"id\" in obj:\n",
        "                obj.pop(\"id\")\n",
        "                removed += 1\n",
        "            new_lines.append(obj)\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for obj in new_lines:\n",
        "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ {path.name}: removed id from {removed} samples\")\n"
      ],
      "metadata": {
        "id": "vfRriNWEhKgo",
        "outputId": "bb802147-2ac0-4611-9ff8-8a84d2065a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ medical_sft_1K_format.jsonl: removed id from 0 samples\n",
            "✅ sharegpt_zh_1K_format.jsonl: removed id from 0 samples\n",
            "✅ numina_cot_sharegpt_data_1k.jsonl: removed id from 1000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "niwvj5S-RHxQ",
        "collapsed": true,
        "outputId": "6a51a310-5b7f-444f-9c27-cd80e5427791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 09:34:48.880376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765877688.901392    6833 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765877688.907774    6833 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765877688.925010    6833 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877688.925037    6833 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877688.925043    6833 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765877688.925046    6833 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-16 09:34:53.785\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m348\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Dec16_09-34-53_913ef8db9873,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-sft-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mtrain files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl']\u001b[0m\n",
            "\u001b[32m2025-12-16 09:34:54.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1meval files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl']\u001b[0m\n",
            "Generating train split: 2000 examples [00:00, 20362.63 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 1831, in _prepare_split_single\n",
            "    writer.write_table(table)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_writer.py\", line 644, in write_table\n",
            "    pa_table = table_cast(pa_table, self._schema)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/table.py\", line 2272, in table_cast\n",
            "    return cast_table_to_schema(table, schema)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/table.py\", line 2218, in cast_table_to_schema\n",
            "    raise CastError(\n",
            "datasets.table.CastError: Couldn't cast\n",
            "id: string\n",
            "conversations: list<item: struct<from: string, value: string>>\n",
            "  child 0, item: struct<from: string, value: string>\n",
            "      child 0, from: string\n",
            "      child 1, value: string\n",
            "to\n",
            "{'conversations': List({'from': Value('string'), 'value': Value('string')})}\n",
            "because column names don't match\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/supervised_finetuning.py\", line 925, in <module>\n",
            "    main()\n",
            "  File \"/content/MedicalGPT/supervised_finetuning.py\", line 418, in main\n",
            "    raw_datasets = load_dataset(\n",
            "                   ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 1412, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 894, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 970, in _download_and_prepare\n",
            "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 1702, in _prepare_split\n",
            "    for job_id, done, content in self._prepare_split_single(\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 1833, in _prepare_split_single\n",
            "    raise DatasetGenerationCastError.from_cast_error(\n",
            "datasets.exceptions.DatasetGenerationCastError: An error occurred while generating the dataset\n",
            "\n",
            "All the data files must have the same columns, but at some point there are 1 new columns ({'id'})\n",
            "\n",
            "This happened while the json dataset builder was generating data using\n",
            "\n",
            "/content/MedicalGPT/./data/finetune/numina_cot_sharegpt_data_1k.jsonl\n",
            "\n",
            "Please either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2nHmAYyRHxQ"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sKk0B_RwRHxR"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "K6zQAzonRHxR"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQFzf_9lRHxR"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOhmlfxRHxR"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7wNjXbcRHxR"
      },
      "outputs": [],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aiWF8j4JRHxR"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "OiEpzDyARHxR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ByqcBIWLRHxR"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yTj8WbcBRHxR"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Zsei_JC7RHxS"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaFl3W7wRHxS"
      },
      "outputs": [],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q__SOEl-RHxS"
      },
      "outputs": [],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMh1QfZiRHxS"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "AjUBaPuNRHxS"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1ve7ePqhRHxS"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGjwlFcoRHxS"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsDiNzjcRHxT"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM3RQqxTRHxT"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qrUmGHAsRHxT"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Prwrq_ciRHxT"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "9eAyVOUcRHxT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pee2Vq50RHxU"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "vX5f_R__RHxU"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kZwu9mH1RHxU"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uqu_TyrRHxU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}